{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV is built with OpenMP support. This usually results in poor performance. For details, see https://github.com/tensorpack/benchmarks/blob/master/ImageNet/benchmark-opencv-resize.py\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyleaves.utils.csv_utils import (process_and_save_singledomain_datasets,\n",
    "                                      process_and_save_multidomain_datasets,\n",
    "                                      process_and_save_multidataset_singledomain_datasets,\n",
    "                                      load_dataset)\n",
    "\n",
    "from pyleaves.leavesdb import db_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying sql db file from /home/jacob/pyleaves/pyleaves/leavesdb/resources/leavesdb.db to /home/jacob/scripts/leavesdb.db\n",
      "Proceeding with sql db at location /home/jacob/scripts/leavesdb.db\n"
     ]
    }
   ],
   "source": [
    "local_db = db_utils.init_local_db()\n",
    "output_dir = r'/media/data_cifs/jacob/Fossil_Project/replication_data'\n",
    "# one_stage_val_splits = {'val_size':0.2, 'test_size':0.2}\n",
    "# two_stage_val_splits = {'source':\n",
    "#                                 {'val_size':0.2, 'test_size':0.0},\n",
    "#                         'target':\n",
    "#                                 {'val_size':0.2, 'test_size':0.2}\n",
    "#                        }\n",
    "\n",
    "one_stage_val_splits = {'val_size':0.0, 'test_size':0.5}\n",
    "two_stage_val_splits = {'source':\n",
    "                                {'val_size':0.0, 'test_size':0.5},\n",
    "                        'target':\n",
    "                                {'val_size':0.0, 'test_size':0.5}\n",
    "                       }\n",
    "\n",
    "\n",
    "\n",
    "dataset_names = ['PNAS','Leaves', 'Fossil']\n",
    "\n",
    "dataset_name_pairs = [\n",
    "                        ['PNAS','Leaves'],\n",
    "                        ['PNAS','Fossil'],\n",
    "                        ['Leaves','PNAS'],\n",
    "                        ['Leaves','Fossil'],\n",
    "                        ['Fossil','PNAS'],\n",
    "                        ['Fossil','Leaves']\n",
    "                     ]\n",
    "\n",
    "dataset_name_merged_pairs = ['PNAS+Leaves','PNAS+Fossil','Leaves+Fossil']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## LOAD FULL DATASETS INTO DICT WITH {dataset_name:dataset_data} format\n",
    "data_dict = {}\n",
    "for name in dataset_names:\n",
    "    data_dict[name] = load_dataset(local_db, name, x_col='path', y_col='family')\n",
    "\n",
    "    \n",
    "# process_and_save_multidataset_singledomain_datasets(data_dict=data_dict,\n",
    "#                                                     dataset_names=dataset_names,\n",
    "#                                                     validation_splits=one_stage_val_splits, \n",
    "#                                                     output_root_dir = os.path.join(output_dir,'2dataset-1domain_experiments'),\n",
    "#                                                     merge_new_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved PNAS dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/PNAS/PNAS/train_data.csv\n",
      "saved PNAS dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/PNAS/PNAS/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/PNAS/PNAS/label_mappings.csv\n",
      "saved Leaves dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/Leaves/Leaves/train_data.csv\n",
      "saved Leaves dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/Leaves/Leaves/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/Leaves/Leaves/label_mappings.csv\n",
      "saved Fossil dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/Fossil/Fossil/train_data.csv\n",
      "saved Fossil dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/Fossil/Fossil/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/single-domain_experiments/Fossil/Fossil/label_mappings.csv\n",
      "saved source_PNAS dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Leaves/source_PNAS/train_data.csv\n",
      "saved source_PNAS dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Leaves/source_PNAS/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Leaves/source_PNAS/label_mappings.csv\n",
      "saved target_Leaves dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Leaves/target_Leaves/train_data.csv\n",
      "saved target_Leaves dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Leaves/target_Leaves/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Leaves/target_Leaves/label_mappings.csv\n",
      "saved source_PNAS dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Fossil/source_PNAS/train_data.csv\n",
      "saved source_PNAS dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Fossil/source_PNAS/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Fossil/source_PNAS/label_mappings.csv\n",
      "saved target_Fossil dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Fossil/target_Fossil/train_data.csv\n",
      "saved target_Fossil dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Fossil/target_Fossil/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/PNAS_Fossil/target_Fossil/label_mappings.csv\n",
      "saved source_Leaves dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_PNAS/source_Leaves/train_data.csv\n",
      "saved source_Leaves dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_PNAS/source_Leaves/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_PNAS/source_Leaves/label_mappings.csv\n",
      "saved target_PNAS dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_PNAS/target_PNAS/train_data.csv\n",
      "saved target_PNAS dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_PNAS/target_PNAS/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_PNAS/target_PNAS/label_mappings.csv\n",
      "saved source_Leaves dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_Fossil/source_Leaves/train_data.csv\n",
      "saved source_Leaves dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_Fossil/source_Leaves/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_Fossil/source_Leaves/label_mappings.csv\n",
      "saved target_Fossil dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_Fossil/target_Fossil/train_data.csv\n",
      "saved target_Fossil dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_Fossil/target_Fossil/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Leaves_Fossil/target_Fossil/label_mappings.csv\n",
      "saved source_Fossil dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_PNAS/source_Fossil/train_data.csv\n",
      "saved source_Fossil dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_PNAS/source_Fossil/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_PNAS/source_Fossil/label_mappings.csv\n",
      "saved target_PNAS dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_PNAS/target_PNAS/train_data.csv\n",
      "saved target_PNAS dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_PNAS/target_PNAS/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_PNAS/target_PNAS/label_mappings.csv\n",
      "saved source_Fossil dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_Leaves/source_Fossil/train_data.csv\n",
      "saved source_Fossil dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_Leaves/source_Fossil/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_Leaves/source_Fossil/label_mappings.csv\n",
      "saved target_Leaves dataset train subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_Leaves/target_Leaves/train_data.csv\n",
      "saved target_Leaves dataset test subset to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_Leaves/target_Leaves/test_data.csv\n",
      "saved label maps to /media/data_cifs/jacob/Fossil_Project/replication_data/2-domain_experiments/Fossil_Leaves/target_Leaves/label_mappings.csv\n"
     ]
    }
   ],
   "source": [
    "process_and_save_singledomain_datasets(data_dict,\n",
    "                                       dataset_names,\n",
    "                                       validation_splits=one_stage_val_splits,\n",
    "                                       output_root_dir=os.path.join(output_dir,'single-domain_experiments'))\n",
    "\n",
    "## PARSE AND DISTRIBUTE DATA FILES FOR MULTIDOMAIN EXPERIMENTS\n",
    "process_and_save_multidomain_datasets(data_dict,\n",
    "                                     dataset_name_pairs,\n",
    "                                     validation_splits=two_stage_val_splits,\n",
    "                                     output_root_dir=os.path.join(output_dir,'2-domain_experiments'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_and_save_multidataset_singledomain_datasets(data_dict: dict, dataset_names: list, validation_splits: dict, output_root_dir: str, merge_new_labels: bool = True):\n",
    "#     '''\n",
    "#     Generate CSV datasets for single domain experiment, but for each pair of datasets to be merged\n",
    "#     Arguments:\n",
    "#         data_dict: dict\n",
    "#         dataset_names: list\n",
    "#             e.g. ['PNAS','Fossil','Leaves']. Pairs will be created within function.\n",
    "#         validation_splits: dict\n",
    "#         output_root_dir: str\n",
    "#         merge_new_labels: bool = True\n",
    "    \n",
    "#     '''\n",
    "#     for i, dataset_1 in enumerate(dataset_names):\n",
    "#         for j, dataset_2 in enumerate(dataset_names):\n",
    "#             if j==i:\n",
    "#                 continue\n",
    "#             dataset_name = '+'.join(dataset_1,dataset_2)\n",
    "        \n",
    "#             encoder = LabelEncoder()\n",
    "#             experiment_dir = os.path.join(output_root_dir, dataset_name)\n",
    "            \n",
    "#             input_data = pd.concat([data_dict[dataset_1], data_dict[dataset_2]])\n",
    "#             process_and_save_dataset(input_data,\n",
    "#                                      name=dataset_name,\n",
    "#                                      encoder=encoder,\n",
    "#                                      validation_splits=validation_splits, \n",
    "#                                      experiment_dir=experiment_dir,\n",
    "#                                      merge_new_labels=merge_new_labels,\n",
    "#                                      other_data_keys=['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-361872f9cbb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                     \u001b[0mvalidation_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mone_stage_val_splits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                     \u001b[0moutput_root_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2dataset-1domain_experiments'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                                                     merge_new_labels = True)\n\u001b[0m",
      "\u001b[0;32m~/pyleaves/pyleaves/utils/csv_utils.py\u001b[0m in \u001b[0;36mprocess_and_save_multidataset_singledomain_datasets\u001b[0;34m(data_dict, dataset_names, validation_splits, output_root_dir, merge_new_labels)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'+'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: join() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "process_and_save_multidataset_singledomain_datasets(data_dict=data_dict,\n",
    "                                                    dataset_names=dataset_names,\n",
    "                                                    validation_splits=one_stage_val_splits, \n",
    "                                                    output_root_dir = os.path.join(output_dir,'2dataset-1domain_experiments'),\n",
    "                                                    merge_new_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying sql db file from /home/jacob/pyleaves/pyleaves/leavesdb/resources/leavesdb.db to /home/jacob/scripts/leavesdb.db\n",
      "Proceeding with sql db at location /home/jacob/scripts/leavesdb.db\n",
      "FOUND:\n",
      "44518 UNIQUE paths with 1 duplicates\n",
      "----------\n",
      "Keeping a total of 44518 paths and discarding 0 duplicates\n"
     ]
    }
   ],
   "source": [
    "import dataset\n",
    "from stuf import stuf\n",
    "import pandas as pd\n",
    "from pyleaves.leavesdb import db_utils, db_query\n",
    "from pyleaves import leavesdb\n",
    "\n",
    "local_db = db_utils.init_local_db()\n",
    "\n",
    "db = dataset.connect(f\"sqlite:///{local_db}\", row_type=stuf)\n",
    "datasets = {}\n",
    "for d in dataset_names:\n",
    "    datasets.update({d:pd.DataFrame(db_query.load_data(db=db, x_col='path', y_col='family', dataset=d))})\n",
    "data = pd.DataFrame(db['dataset'].all())\n",
    "leavesdb.db_manager.analyze_db_contents(local_db)\n",
    "\n",
    "# print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting only samples that belong to a class with population >= 20 samples\n",
      "Previous num_classes = 376, new num_classes = 144\n",
      "Previous data_df.shape = (26953, 2), new data_df.shape = (25660, 2)\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "from pyleaves.data_pipeline import preprocessing\n",
    "import numpy as np\n",
    "# for k,v in datasets.items():\n",
    "#     print(k, v.shape)\n",
    "\n",
    "\n",
    "filtered_data = preprocessing.filter_low_count_labels(datasets['Leaves'], threshold=20)\n",
    "\n",
    "print(len(np.unique(filtered_data['family'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND:\n",
      "44518 UNIQUE paths with 1 duplicates\n",
      "----------\n",
      "Keeping a total of 44518 paths and discarding 0 duplicates\n"
     ]
    }
   ],
   "source": [
    "leavesdb.db_manager.analyze_db_contents(local_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fossil', 27), ('Leaves', 376), ('PNAS', 19), ('plant_village', 3)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_utils.__get_num_families_per_dataset(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Database column keys': ['id', 'species', 'genus', 'path', 'family', 'dataset', 'label', 'source_path'], 'distinct datasets': ['Fossil', 'Leaves', 'PNAS', 'plant_village'], 'Number of distinct families': [('Fossil', 27), ('Leaves', 376), ('PNAS', 19), ('plant_village', 3)], 'Number of rows in db': 44518}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Database column keys': ['id',\n",
       "  'species',\n",
       "  'genus',\n",
       "  'path',\n",
       "  'family',\n",
       "  'dataset',\n",
       "  'label',\n",
       "  'source_path'],\n",
       " 'distinct datasets': ['Fossil', 'Leaves', 'PNAS', 'plant_village'],\n",
       " 'Number of distinct families': [('Fossil', 27),\n",
       "  ('Leaves', 376),\n",
       "  ('PNAS', 19),\n",
       "  ('plant_village', 3)],\n",
       " 'Number of rows in db': 44518}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_utils.summarize_db(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying sql db file from /home/jacob/pyleaves/pyleaves/leavesdb/resources/leavesdb.db to /home/jacob/scripts/leavesdb.db\n",
      "Proceeding with sql db at location /home/jacob/scripts/leavesdb.db\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>family</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>/media/data_cifs/jacob/Fossil_Project/opt_data...</td>\n",
       "      <td>Adoxaceae</td>\n",
       "      <td>Fossil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5315</th>\n",
       "      <td>/media/data_cifs/jacob/Fossil_Project/opt_data...</td>\n",
       "      <td>Adoxaceae</td>\n",
       "      <td>Fossil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5316</th>\n",
       "      <td>/media/data_cifs/jacob/Fossil_Project/opt_data...</td>\n",
       "      <td>Adoxaceae</td>\n",
       "      <td>Fossil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5317</th>\n",
       "      <td>/media/data_cifs/jacob/Fossil_Project/opt_data...</td>\n",
       "      <td>Adoxaceae</td>\n",
       "      <td>Fossil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>/media/data_cifs/jacob/Fossil_Project/opt_data...</td>\n",
       "      <td>Adoxaceae</td>\n",
       "      <td>Fossil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path     family dataset\n",
       "5314  /media/data_cifs/jacob/Fossil_Project/opt_data...  Adoxaceae  Fossil\n",
       "5315  /media/data_cifs/jacob/Fossil_Project/opt_data...  Adoxaceae  Fossil\n",
       "5316  /media/data_cifs/jacob/Fossil_Project/opt_data...  Adoxaceae  Fossil\n",
       "5317  /media/data_cifs/jacob/Fossil_Project/opt_data...  Adoxaceae  Fossil\n",
       "5318  /media/data_cifs/jacob/Fossil_Project/opt_data...  Adoxaceae  Fossil"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataset\n",
    "import numpy as np\n",
    "from stuf import stuf\n",
    "import pandas as pd\n",
    "from pyleaves.leavesdb import db_utils, db_query\n",
    "from pyleaves import leavesdb\n",
    "\n",
    "def load_data(db,\n",
    "              datasets=['Fossil','Leaves'],\n",
    "              x_col='path',\n",
    "              y_col='family',\n",
    "              keep_cols=['dataset']              \n",
    "              ):\n",
    "    data_df = pd.DataFrame(db['dataset'].all())\n",
    "    data = []\n",
    "    columns = [x_col, y_col, *keep_cols]\n",
    "    #         data[data['dataset'] in datasets]\n",
    "    for name in datasets:\n",
    "        data += [\n",
    "                    data_df[data_df.loc[:,'dataset'] == name]\n",
    "                ]\n",
    "    data = pd.concat(data)\n",
    "    data = data.loc[:,columns]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "x_col='path'\n",
    "y_col='family'\n",
    "keep_cols=['dataset']\n",
    "datasets=['Fossil','Leaves']\n",
    "\n",
    "\n",
    "\n",
    "local_db = db_utils.init_local_db()\n",
    "db = dataset.connect(f\"sqlite:///{local_db}\", row_type=stuf)\n",
    "\n",
    "data = load_data(db,\n",
    "              datasets=['Fossil','Leaves', 'PNAS'],\n",
    "              x_col='path',\n",
    "              y_col='family',\n",
    "              keep_cols=['dataset']              \n",
    "              )\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path       6122\n",
      "family       27\n",
      "dataset       1\n",
      "dtype: int64\n",
      "11\n",
      "path       26953\n",
      "family       376\n",
      "dataset        1\n",
      "dtype: int64\n",
      "62\n",
      "path       5314\n",
      "family       19\n",
      "dataset       1\n",
      "dtype: int64\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "for d in data.groupby('dataset'):\n",
    "    l, count = np.unique(d[1], return_counts=True)\n",
    "    print(d[1].nunique())\n",
    "    print(len(l[count>=100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Fossil', 'Leaves', 'PNAS'], dtype=object),\n",
       " array([ 6122, 26953,  5314]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data['dataset'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

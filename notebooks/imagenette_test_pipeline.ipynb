{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "import os\n",
    "from pprint import pprint\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4'\n",
    "import tensorflow as tf\n",
    "random.seed(84)\n",
    "np.random.seed(58)\n",
    "tf.random.set_seed(34)\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, TensorBoard, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_datasets as tfds\n",
    "from pyleaves.datasets import leaves_dataset, fossil_dataset, pnas_dataset, base_dataset\n",
    "import neptune\n",
    "import arrow\n",
    "from pyleaves.utils import ensure_dir_exists\n",
    "from more_itertools import unzip\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "def image_reshape(x):\n",
    "    return [\n",
    "        tf.image.resize(x, (7, 7)),\n",
    "        tf.image.resize(x, (14, 14)),\n",
    "        x\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_img(image_path):#, img_size=(224,224)):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize(image, h=512, w=512):\n",
    "    return tf.image.resize_with_pad(image, target_height=h, target_width=w)\n",
    "\n",
    "\n",
    "\n",
    "def _cond_apply(x, y, func, prob):\n",
    "    \"\"\"Conditionally apply func to x and y with probability prob.\n",
    "    \"\"\"\n",
    "    return tf.cond((tf.random.uniform([], 0, 1) >= (1.0 - prob)), lambda: func(x,y), lambda: (x,y))\n",
    "\n",
    "\n",
    "def augment_sample(x, y):\n",
    "    x = tf.image.random_flip_left_right(x, seed=2)\n",
    "    x = tf.image.random_flip_up_down(x, seed=2)\n",
    "    return x, y\n",
    "                \n",
    "\n",
    "\n",
    "def create_mnist_dataset(batch_size):\n",
    "    (train_images, _), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    train_images = train_images.reshape([-1, 28, 28, 1]).astype('float32')\n",
    "    train_images = train_images/127.5  - 1\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "    dataset = dataset.map(image_reshape)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(len(train_images))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prep_dataset(dataset, batch_size=32, buffer_size=100, shuffle=False, target_size=(512,512), augment=False, aug_prob=1.0):\n",
    "    dataset = dataset.map(lambda x,y: (resize(x, *target_size),y), num_parallel_calls=-1)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(lambda x,y: _cond_apply(x, y, augment_sample, prob=aug_prob), num_parallel_calls=-1)\n",
    "#         dataset = dataset.map(augment_sample, num_parallel_calls=-1)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_Imagenette_dataset(batch_size,\n",
    "                              target_size=(512,512), \n",
    "                              augment_train=True,\n",
    "                              aug_prob=1.0):\n",
    "    \n",
    "    data, info = tfds.load('Imagenette', as_supervised=True, with_info=True)\n",
    "    train_data = prep_dataset(data['train'], \n",
    "                              batch_size=batch_size, \n",
    "                              buffer_size=info.splits['train'].num_examples,\n",
    "                              shuffle=True,\n",
    "                              target_size=target_size,\n",
    "                              augment=augment_train,\n",
    "                              aug_prob=aug_prob)\n",
    "    val_data = prep_dataset(data['validation'], \n",
    "                            batch_size=batch_size,\n",
    "                            target_size=target_size)\n",
    "    \n",
    "    return train_data, val_data, info\n",
    "\n",
    "\n",
    "\n",
    "def partition_data(data, partitions=OrderedDict({'train':0.5,'test':0.5})):\n",
    "    '''\n",
    "    Split data into named partitions by fraction\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >> split_data = partition_data(data, partitions=OrderedDict({'train':0.4,'val':0.1,'test':0.5}))\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    output={}\n",
    "    taken = 0.0\n",
    "    for k,v in partitions.items():\n",
    "        idx = (int(taken*num_rows),int((taken+v)*num_rows))\n",
    "        output.update({k:data[idx[0]:idx[1]]})\n",
    "        taken+=v\n",
    "    assert taken <= 1.0\n",
    "    return output\n",
    "\n",
    "def load_pnas_data(splits={'train':0.7,'validation':0.3}):\n",
    "    data_files = pnas_dataset.PNASDataset()\n",
    "    \n",
    "    data_files.exclude_rare_classes(threshold=50)\n",
    "    encoder = base_dataset.LabelEncoder(data_files.classes)\n",
    "    data_files, _ = data_files.enforce_class_whitelist(class_names=encoder.classes)\n",
    "\n",
    "    x = list(data_files.data['path'].values)#.reshape((-1,1))\n",
    "    y = np.array(encoder.encode(data_files.data['family']))\n",
    "\n",
    "    shuffled_data = list(zip(x,y))\n",
    "    random.shuffle(shuffled_data)\n",
    "    partitioned_data = partition_data(data=shuffled_data,\n",
    "                                      partitions=OrderedDict(splits))\n",
    "    split_data = {k:v for k,v in partitioned_data.items() if len(v)>0}\n",
    "    \n",
    "    for subset, subset_data in split_data.items():\n",
    "        split_data[subset] = [list(i) for i in unzip(subset_data)]\n",
    "    \n",
    "    paths = tf.data.Dataset.from_tensor_slices(split_data['train'][0])\n",
    "    labels = tf.data.Dataset.from_tensor_slices(split_data['train'][1])\n",
    "    train_data = tf.data.Dataset.zip((paths, labels))\n",
    "    train_data = train_data.map(lambda x,y: (tf.image.convert_image_dtype(load_img(x)*255.0,dtype=tf.uint8),y), num_parallel_calls=-1)\n",
    "    \n",
    "    paths = tf.data.Dataset.from_tensor_slices(split_data['validation'][0])\n",
    "    labels = tf.data.Dataset.from_tensor_slices(split_data['validation'][1])\n",
    "    validation_data = tf.data.Dataset.zip((paths, labels))\n",
    "    validation_data = validation_data.map(lambda x,y: (tf.image.convert_image_dtype(load_img(x)*255.0,dtype=tf.uint8),y), num_parallel_calls=-1)\n",
    "    \n",
    "    return {'train':train_data, \n",
    "            'validation':validation_data}, data_files\n",
    "\n",
    "\n",
    "def create_pnas_dataset(batch_size, \n",
    "                        target_size=(512,512),\n",
    "                        splits={'train':0.7,'validation':0.3},\n",
    "                        augment_train=True,\n",
    "                        aug_prob=1.0):\n",
    "    \n",
    "    dataset, data_files = load_pnas_data(splits=splits)\n",
    "    train_data = prep_dataset(dataset['train'], \n",
    "                              batch_size=batch_size, \n",
    "                              buffer_size=int(data_files.num_samples*splits['train']),\n",
    "                              shuffle=True,\n",
    "                              target_size=target_size,\n",
    "                              augment=augment_train,\n",
    "                              aug_prob=aug_prob)\n",
    "    val_data = prep_dataset(dataset['validation'], \n",
    "                            batch_size=batch_size,\n",
    "                            target_size=target_size)\n",
    "    return train_data, val_data, data_files\n",
    "\n",
    "\n",
    "# def create_pnas_dataset(batch_size):\n",
    "#     data_files = pnas_dataset.PNASDataset()\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(data_files.data['path'])\n",
    "#     dataset = dataset.map(load_img, num_parallel_calls=-1)\n",
    "#     dataset = dataset.map(resize, num_parallel_calls=-1)\n",
    "#     dataset = dataset.cache()\n",
    "#     dataset = dataset.shuffle(20)#len(data_files.data))\n",
    "#     dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "#     dataset = dataset.prefetch(1)\n",
    "#     return dataset\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "    \n",
    "preprocess_input(tf.zeros([4, 224, 224, 3]))\n",
    "def apply_preprocess(x, y, num_classes=10):\n",
    "    return preprocess_input(x), tf.one_hot(y, depth=num_classes)\n",
    "    \n",
    "\n",
    "def build_head(base, num_classes=10):\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    dense1 = tf.keras.layers.Dense(2048,activation='relu',name='dense1')#, kernel_initializer=tf.initializers.GlorotNormal())\n",
    "    dense2 = tf.keras.layers.Dense(512,activation='relu',name='dense2')#, kernel_initializer=tf.initializers.GlorotNormal())\n",
    "    prediction_layer = tf.keras.layers.Dense(num_classes,activation='softmax')#, kernel_initializer=tf.initializers.GlorotNormal())\n",
    "    model = tf.keras.Sequential([\n",
    "        base,\n",
    "        global_average_layer,dense1,dense2,\n",
    "        prediction_layer\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def log_data(logs):\n",
    "    for k, v in logs.items():\n",
    "        neptune.log_metric(k, v)\n",
    "\n",
    "        \n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "        \n",
    "def plot_sample(sample, num_res=1):\n",
    "    num_samples = min(64, len(sample[0]))\n",
    "\n",
    "    grid = gridspec.GridSpec(num_res, num_samples)\n",
    "    grid.update(left=0, bottom=0, top=1, right=1, wspace=0.01, hspace=0.01)\n",
    "    fig = plt.figure(figsize=[num_samples, num_res])\n",
    "    for x in range(num_res):\n",
    "        images = sample[x].numpy() #this converts the tensor to a numpy array\n",
    "        images = np.squeeze(images)\n",
    "        for y in range(num_samples):\n",
    "            ax = fig.add_subplot(grid[x, y])\n",
    "            ax.set_axis_off()\n",
    "            ax.imshow((images[y] + 1.0)/2, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# neptune_logger = tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_data(logs))\n",
    "neptune_logger = tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: log_data(logs),\n",
    "                                                  on_epoch_end=lambda epoch, logs: log_data(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyleaves.utils.neptune_utils import ImageLoggerCallback\n",
    "\n",
    "def train_imagenette(PARAMS):\n",
    "\n",
    "    neptune.append_tag(PARAMS['dataset_name'])\n",
    "    neptune.append_tag(PARAMS['model_name'])\n",
    "    \n",
    "    K.clear_session()\n",
    "    tf.random.set_seed(34)\n",
    "    target_size = PARAMS['target_size']\n",
    "    BATCH_SIZE = PARAMS['BATCH_SIZE']\n",
    "    \n",
    "    train_dataset, validation_dataset, info = create_Imagenette_dataset(BATCH_SIZE, target_size=target_size, augment_train=PARAMS['augment_train'])\n",
    "    num_classes = info.features['label'].num_classes\n",
    "    \n",
    "    encoder = base_dataset.LabelEncoder(info.features['label'].names)\n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x,y: apply_preprocess(x,y,num_classes),num_parallel_calls=-1)\n",
    "    validation_dataset = validation_dataset.map(lambda x,y: apply_preprocess(x,y,num_classes),num_parallel_calls=-1)\n",
    "    \n",
    "    PARAMS['num_classes'] = num_classes\n",
    "    steps_per_epoch = info.splits['train'].num_examples//BATCH_SIZE\n",
    "    validation_steps = info.splits['validation'].num_examples//BATCH_SIZE\n",
    "\n",
    "    neptune.set_property('num_classes',num_classes)\n",
    "    neptune.set_property('steps_per_epoch',steps_per_epoch)\n",
    "    neptune.set_property('validation_steps',validation_steps)\n",
    "\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=PARAMS['learning_rate'])\n",
    "    loss = 'categorical_crossentropy'\n",
    "    METRICS = ['accuracy']\n",
    "\n",
    "    base = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                             include_top=False,\n",
    "                                             input_tensor=Input(shape=(*target_size,3)))\n",
    "\n",
    "    # TODO try freezing weights for input_shape != (224,224)\n",
    "\n",
    "    model = build_head(base, num_classes=num_classes)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=METRICS)\n",
    "\n",
    "    callbacks = [neptune_logger,\n",
    "                 ImageLoggerCallback(data=train_dataset, freq=10, max_images=-1, name='train', encoder=encoder),\n",
    "                 ImageLoggerCallback(data=validation_dataset, freq=10, max_images=-1, name='val', encoder=encoder),\n",
    "                 EarlyStopping(monitor='val_loss', patience=2, verbose=1)]\n",
    "\n",
    "    model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "    pprint(PARAMS)\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=10,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=validation_dataset,\n",
    "                        shuffle=True,\n",
    "                        initial_epoch=0,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps)\n",
    "    \n",
    "    \n",
    "def train_pnas(PARAMS):\n",
    "    ensure_dir_exists(PARAMS['log_dir'])\n",
    "    neptune.append_tag(PARAMS['dataset_name'])\n",
    "    neptune.append_tag(PARAMS['model_name'])\n",
    "    \n",
    "    K.clear_session()\n",
    "    tf.random.set_seed(34)\n",
    "    \n",
    "    splits = PARAMS['splits']\n",
    "    train_dataset, validation_dataset, data_files = create_pnas_dataset(batch_size=PARAMS['BATCH_SIZE'],\n",
    "                                                                        target_size=PARAMS['target_size'],\n",
    "                                                                        splits=splits,\n",
    "                                                                        augment_train=PARAMS['augment_train'])\n",
    "    \n",
    "    PARAMS['num_classes'] = data_files.num_classes #info.features['label'].num_classes\n",
    "    PARAMS['splits_size'] = {'train':{},\n",
    "                       'validation':{}}\n",
    "    PARAMS['splits_size']['train'] = data_files.num_samples*PARAMS['splits']['train']\n",
    "    PARAMS['splits_size']['validation'] = data_files.num_samples*PARAMS['splits']['validation']\n",
    "\n",
    "\n",
    "    steps_per_epoch = PARAMS['splits_size']['train']//PARAMS['BATCH_SIZE']\n",
    "    validation_steps = PARAMS['splits_size']['validation']//PARAMS['BATCH_SIZE']\n",
    "\n",
    "    \n",
    "#     num_classes = info.num_classes\n",
    "#     num_samples = info.num_samples\n",
    "    \n",
    "    encoder = base_dataset.LabelEncoder(data_files.classes)\n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x,y: apply_preprocess(x,y,PARAMS['num_classes']),num_parallel_calls=-1)\n",
    "    validation_dataset = validation_dataset.map(lambda x,y: apply_preprocess(x,y,PARAMS['num_classes']),num_parallel_calls=-1)\n",
    "    \n",
    "#     PARAMS['num_classes'] = num_classes\n",
    "#     steps_per_epoch = int(num_samples*splits['train'])//PARAMS['BATCH_SIZE']\n",
    "#     validation_steps = int(num_samples*splits['validation'])//PARAMS['BATCH_SIZE']\n",
    "\n",
    "    neptune.set_property('num_classes',PARAMS['num_classes'])\n",
    "    neptune.set_property('steps_per_epoch',steps_per_epoch)\n",
    "    neptune.set_property('validation_steps',validation_steps)\n",
    "\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=PARAMS['lr'])\n",
    "\n",
    "    base = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                             include_top=False,\n",
    "                                             input_tensor=Input(shape=(*PARAMS['target_size'],3)))\n",
    "\n",
    "    model = build_head(base, num_classes=PARAMS['num_classes'])\n",
    "\n",
    "    METRICS = ['accuracy']\n",
    "    model.compile(optimizer=PARAMS['optimizer'],\n",
    "                  loss=PARAMS['loss'],\n",
    "                  metrics=METRICS)\n",
    "    \n",
    "\n",
    "    callbacks = [neptune_logger,\n",
    "                 ImageLoggerCallback(data=train_dataset, freq=10, max_images=-1, name='train', encoder=encoder),\n",
    "                 ImageLoggerCallback(data=validation_dataset, freq=10, max_images=-1, name='val', encoder=encoder),\n",
    "                 EarlyStopping(monitor='val_loss', patience=25, verbose=1)]\n",
    "\n",
    "    model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "    pprint(PARAMS)\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=PARAMS['num_epochs'],\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=validation_dataset,\n",
    "                        shuffle=True,\n",
    "                        initial_epoch=0,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PARAMS = {'neptune_project_name':'jacobarose/sandbox',\n",
    "#           'experiment_name':'imagenette_test',\n",
    "#           'experiment_dir':'/media/data/jacob/sandbox_logs',\n",
    "#           'experiment_start_time':arrow.utcnow().format('YYYY-MM-DD_HH-mm-ss'),\n",
    "#           'optimizer':'Adam',\n",
    "#           'loss':'categorical_crossentropy',\n",
    "#           'lr':1e-6,\n",
    "#           'target_size':(224,224), #(256,256),\n",
    "#           'BATCH_SIZE':48,\n",
    "#           'num_epochs':150}\n",
    "\n",
    "# PARAMS['log_dir'] = os.path.join(PARAMS['experiment_dir'], PARAMS['experiment_name'], 'log_dir', PARAMS['loss'], PARAMS['experiment_start_time'])\n",
    "# neptune.init(project_qualified_name=PARAMS['neptune_project_name'])\n",
    "# neptune_tb.integrate_with_tensorflow()\n",
    "\n",
    "\n",
    "# PARAMS['target_size'] = (256,256)\n",
    "# PARAMS['BATCH_SIZE'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset, validation_dataset, info = create_Imagenette_dataset(16, target_size=(224,224), augment_train=True)\n",
    "# a = next(iter(train_data))\n",
    "# print(a[0].numpy().min(),a[0].numpy().max())\n",
    "\n",
    "# train_data, validation_data, info = create_pnas_dataset(batch_size=16,\n",
    "#                                                   target_size=(224,224),\n",
    "#                                                   splits={'train':0.7,'validation':0.3}, \n",
    "#                                                   augment_train=True)\n",
    "# a = next(iter(train_data))\n",
    "# print(a[0].numpy().min(),a[0].numpy().max())\n",
    "# a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ensure_dir_exists(PARAMS['log_dir'])\n",
    "\n",
    "# train_dataset, validation_dataset, info = create_Imagenette_dataset(PARAMS['BATCH_SIZE'], \n",
    "#                                                                     target_size=PARAMS['target_size'])\n",
    "# PARAMS['num_classes'] = info.features['label'].num_classes\n",
    "# PARAMS['splits_size'] = {'train':{},\n",
    "#                    'validation':{}}\n",
    "# PARAMS['splits_size']['train'] = info.splits['train'].num_examples\n",
    "# PARAMS['splits_size']['validation'] = info.splits['validation'].num_examples\n",
    "\n",
    "\n",
    "# steps_per_epoch = PARAMS['splits_size']['train']//PARAMS['BATCH_SIZE']\n",
    "# validation_steps = PARAMS['splits_size']['validation']//PARAMS['BATCH_SIZE']\n",
    "# # steps_per_epoch = info.splits['train'].num_examples//PARAMS['BATCH_SIZE']\n",
    "# # validation_steps = info.splits['validation'].num_examples//PARAMS['BATCH_SIZE']\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=PARAMS['lr'])\n",
    "\n",
    "# big_base = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "#                                              include_top=False,\n",
    "#                                              input_tensor=Input(shape=(*PARAMS['target_size'],3)))\n",
    "\n",
    "# big_model = build_head(big_base, num_classes=PARAMS['num_classes'])\n",
    "\n",
    "\n",
    "# METRICS = ['accuracy']\n",
    "# big_model.compile(optimizer=PARAMS['optimizer'],\n",
    "#               loss=PARAMS['loss'],\n",
    "#               metrics=METRICS)\n",
    "\n",
    "# big_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# target_size = (224,224)\n",
    "\n",
    "# PARAMS['target_size'] = target_size\n",
    "# with neptune.create_experiment(name=experiment_name+'-'+str(target_size), params=PARAMS):\n",
    "\n",
    "#     train_dataset, validation_dataset, info = create_Imagenette_dataset(BATCH_SIZE, target_size=target_size)\n",
    "#     num_classes = info.features['label'].num_classes\n",
    "\n",
    "#     train_dataset = train_dataset.map(lambda x,y: apply_preprocess(x,y,num_classes),num_parallel_calls=-1)\n",
    "#     validation_dataset = validation_dataset.map(lambda x,y: apply_preprocess(x,y,num_classes),num_parallel_calls=-1)\n",
    "\n",
    "\n",
    "#     steps_per_epoch = info.splits['train'].num_examples//BATCH_SIZE\n",
    "#     validation_steps = info.splits['validation'].num_examples//BATCH_SIZE\n",
    "\n",
    "#     loss = 'categorical_crossentropy'\n",
    "#     METRICS = ['accuracy']\n",
    "\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "\n",
    "#     base = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "#                                                     include_top=False,\n",
    "#                                                     input_tensor=Input(shape=(*target_size,3)))\n",
    "\n",
    "#     model = build_head(base, num_classes=num_classes)\n",
    "\n",
    "#     model.compile(optimizer=optimizer,\n",
    "#                   loss=loss,\n",
    "#                   metrics=METRICS)\n",
    "\n",
    "#     callbacks = [neptune_logger,\n",
    "#                  EarlyStopping(monitor='val_loss', patience=5, verbose=1)]\n",
    "\n",
    "    \n",
    "#     model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "    \n",
    "#     history = model.fit(train_dataset,\n",
    "#                         epochs=10,\n",
    "#                         callbacks=callbacks,\n",
    "#                         validation_data=validation_dataset,\n",
    "#                         shuffle=True,\n",
    "#                         initial_epoch=0,\n",
    "#                         steps_per_epoch=steps_per_epoch,\n",
    "#                         validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# K.clear_session()\n",
    "\n",
    "# big_target_size = (256,256)\n",
    "# PARAMS['target_size'] = big_target_size\n",
    "# with neptune.create_experiment(name=experiment_name+'-'+str(big_target_size), params=PARAMS):\n",
    "#     BATCH_SIZE = 16\n",
    "#     train_dataset, validation_dataset, info = create_Imagenette_dataset(BATCH_SIZE, target_size=big_target_size)\n",
    "#     num_classes = info.features['label'].num_classes\n",
    "    \n",
    "\n",
    "#     steps_per_epoch = info.splits['train'].num_examples//BATCH_SIZE\n",
    "#     validation_steps = info.splits['validation'].num_examples//BATCH_SIZE\n",
    "\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "\n",
    "#     big_base = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "#                                                  include_top=False,\n",
    "#                                                  input_tensor=Input(shape=(*big_target_size,3)))\n",
    "\n",
    "#     big_model = build_head(big_base, num_classes=num_classes)\n",
    "\n",
    "#     big_model.compile(optimizer=optimizer,\n",
    "#                   loss=loss,\n",
    "#                   metrics=METRICS)\n",
    "\n",
    "#     callbacks = [neptune_logger,\n",
    "#                  EarlyStopping(monitor='val_loss', patience=2, verbose=1)]\n",
    "\n",
    "#     history = big_model.fit(train_dataset,\n",
    "#                         epochs=10,\n",
    "#                         callbacks=callbacks,\n",
    "#                         validation_data=validation_dataset,\n",
    "#                         shuffle=True,\n",
    "#                         initial_epoch=0,\n",
    "#                         steps_per_epoch=steps_per_epoch,\n",
    "#                         validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There is a new version of neptune-client 0.4.116 (installed: 0.4.115).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/jacobarose/sandbox/e/SAN-434\n",
      "{'BATCH_SIZE': 48,\n",
      " 'augment_train': True,\n",
      " 'dataset_name': 'pnas',\n",
      " 'experiment_dir': '/media/data/jacob/sandbox_logs',\n",
      " 'experiment_name': 'pnas_test',\n",
      " 'experiment_start_time': '2020-07-01_05-58-58',\n",
      " 'log_dir': '/media/data/jacob/sandbox_logs/pnas_test/log_dir/categorical_crossentropy/2020-07-01_05-58-58',\n",
      " 'loss': 'categorical_crossentropy',\n",
      " 'lr': 1e-06,\n",
      " 'model_name': 'vgg16',\n",
      " 'neptune_project_name': 'jacobarose/sandbox',\n",
      " 'num_classes': 19,\n",
      " 'num_epochs': 150,\n",
      " 'optimizer': 'Adam',\n",
      " 'splits': {'train': 0.5, 'validation': 0.5},\n",
      " 'splits_size': {'train': 2657.0, 'validation': 2657.0},\n",
      " 'target_size': (224, 224)}\n",
      "Train for 55.0 steps, validate for 55.0 steps\n",
      "Epoch 1/150\n",
      "Batch 1: Logged 48 train images to neptune\n",
      "Batch 1: Logged 48 val images to neptune\n",
      "10/55 [====>.........................] - ETA: 3:41 - loss: 11.1833 - accuracy: 0.0812Batch 11: Logged 48 train images to neptune\n",
      "Batch 11: Logged 48 val images to neptune\n",
      "20/55 [=========>....................] - ETA: 1:58 - loss: 7.0655 - accuracy: 0.0875Batch 21: Logged 48 train images to neptune\n",
      "Batch 21: Logged 48 val images to neptune\n",
      "30/55 [===============>..............] - ETA: 1:09 - loss: 5.6863 - accuracy: 0.0951Batch 31: Logged 48 train images to neptune\n",
      "Batch 31: Logged 48 val images to neptune\n",
      "40/55 [====================>.........] - ETA: 33s - loss: 4.9871 - accuracy: 0.1156Batch 41: Logged 48 train images to neptune\n",
      "Batch 41: Logged 48 val images to neptune\n",
      "50/55 [==========================>...] - ETA: 9s - loss: 4.5622 - accuracy: 0.1221 Batch 51: Logged 48 train images to neptune\n",
      "Batch 51: Logged 48 val images to neptune\n",
      "55/55 [==============================] - 112s 2s/step - loss: 4.4040 - accuracy: 0.1235 - val_loss: 2.8333 - val_accuracy: 0.1447\n",
      "Epoch 2/150\n",
      "55/55 [==============================] - 27s 496ms/step - loss: 2.8051 - accuracy: 0.1375 - val_loss: 2.8412 - val_accuracy: 0.1492\n",
      "Epoch 3/150\n",
      "55/55 [==============================] - 27s 496ms/step - loss: 2.8009 - accuracy: 0.1481 - val_loss: 2.8256 - val_accuracy: 0.1447\n",
      "Epoch 4/150\n",
      "55/55 [==============================] - 28s 502ms/step - loss: 2.7936 - accuracy: 0.1477 - val_loss: 2.8251 - val_accuracy: 0.1447\n",
      "Epoch 5/150\n",
      "55/55 [==============================] - 28s 511ms/step - loss: 2.7995 - accuracy: 0.1383 - val_loss: 2.8269 - val_accuracy: 0.1447\n",
      "Epoch 6/150\n",
      "55/55 [==============================] - 28s 517ms/step - loss: 2.8019 - accuracy: 0.1470 - val_loss: 2.8277 - val_accuracy: 0.1447\n",
      "Epoch 7/150\n",
      "55/55 [==============================] - 28s 505ms/step - loss: 2.7979 - accuracy: 0.1473 - val_loss: 2.8248 - val_accuracy: 0.1447\n",
      "Epoch 8/150\n",
      "55/55 [==============================] - 28s 515ms/step - loss: 2.8009 - accuracy: 0.1504 - val_loss: 2.8229 - val_accuracy: 0.1447\n",
      "Epoch 9/150\n",
      "55/55 [==============================] - 28s 508ms/step - loss: 2.7816 - accuracy: 0.1561 - val_loss: 2.8284 - val_accuracy: 0.1447\n",
      "Epoch 10/150\n",
      "55/55 [==============================] - 28s 507ms/step - loss: 2.8019 - accuracy: 0.1360 - val_loss: 2.8241 - val_accuracy: 0.1447\n",
      "Epoch 11/150\n",
      "55/55 [==============================] - 28s 507ms/step - loss: 2.7922 - accuracy: 0.1549 - val_loss: 2.8206 - val_accuracy: 0.1447\n",
      "Epoch 12/150\n",
      "55/55 [==============================] - 28s 511ms/step - loss: 2.7964 - accuracy: 0.1466 - val_loss: 2.8243 - val_accuracy: 0.1447\n",
      "Epoch 13/150\n",
      "55/55 [==============================] - 28s 516ms/step - loss: 2.7924 - accuracy: 0.1424 - val_loss: 2.8254 - val_accuracy: 0.1447\n",
      "Epoch 14/150\n",
      "55/55 [==============================] - 28s 517ms/step - loss: 2.7970 - accuracy: 0.1504 - val_loss: 2.8227 - val_accuracy: 0.1447\n",
      "Epoch 15/150\n",
      "55/55 [==============================] - 29s 521ms/step - loss: 2.7929 - accuracy: 0.1527 - val_loss: 2.8213 - val_accuracy: 0.1447\n",
      "Epoch 16/150\n",
      "55/55 [==============================] - 28s 510ms/step - loss: 2.7936 - accuracy: 0.1432 - val_loss: 2.8245 - val_accuracy: 0.1447\n",
      "Epoch 17/150\n",
      "55/55 [==============================] - 29s 526ms/step - loss: 2.7909 - accuracy: 0.1508 - val_loss: 2.8248 - val_accuracy: 0.1447\n",
      "Epoch 18/150\n",
      "54/55 [============================>.] - ETA: 0s - loss: 2.7972 - accuracy: 0.1505"
     ]
    }
   ],
   "source": [
    "# PARAMS = {'neptune_project_name':'jacobarose/sandbox',\n",
    "#           'experiment_name':'pnas_test',\n",
    "#           'loss':'Adam'}\n",
    "\n",
    "PARAMS = {'neptune_project_name':'jacobarose/sandbox',\n",
    "          'experiment_name':'pnas_test',\n",
    "          'experiment_dir':'/media/data/jacob/sandbox_logs',\n",
    "          'experiment_start_time':arrow.utcnow().format('YYYY-MM-DD_HH-mm-ss'),\n",
    "          'optimizer':'Adam',\n",
    "          'loss':'categorical_crossentropy',\n",
    "          'lr':1e-6,\n",
    "          'target_size':(224,224), #(256,256),\n",
    "          'BATCH_SIZE':48,\n",
    "          'num_epochs':150,\n",
    "          'dataset_name':'pnas',\n",
    "          'model_name':'vgg16',\n",
    "          'augment_train':True,\n",
    "          'splits':{'train':0.5,'validation':0.5}}\n",
    "\n",
    "PARAMS['log_dir'] = os.path.join(PARAMS['experiment_dir'], PARAMS['experiment_name'], 'log_dir', PARAMS['loss'], PARAMS['experiment_start_time'])\n",
    "\n",
    "\n",
    "# PARAMS['learning_rate'] = 1e-6\n",
    "# PARAMS['target_size'] = (224,224)\n",
    "# PARAMS['BATCH_SIZE'] = 48\n",
    "# PARAMS['num_epochs'] = 150\n",
    "# PARAMS['dataset_name'] = 'pnas'\n",
    "# PARAMS['model_name'] = 'vgg16'\n",
    "# PARAMS['augment_train'] = True\n",
    "# PARAMS['splits'] = {'train':0.5,'validation':0.5}\n",
    "\n",
    "neptune.init(project_qualified_name=PARAMS['neptune_project_name'])\n",
    "with neptune.create_experiment(name=PARAMS['experiment_name']+'-'+str(PARAMS['splits']), params=PARAMS):\n",
    "    train_pnas(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {'neptune_project_name':'jacobarose/sandbox',\n",
    "          'experiment_name':'imagenette_test',\n",
    "          'loss':'Adam'}\n",
    "\n",
    "\n",
    "PARAMS['learning_rate'] = 1e-6\n",
    "PARAMS['target_size'] = (224,224)\n",
    "PARAMS['BATCH_SIZE'] = 16\n",
    "PARAMS['dataset_name'] = 'imagenette'\n",
    "PARAMS['model_name'] = 'vgg16'\n",
    "PARAMS['augment_train'] = True\n",
    "PARAMS['splits'] = {'train':0.7,'validation':0.3}\n",
    "\n",
    "neptune.init(project_qualified_name=PARAMS['neptune_project_name'])\n",
    "with neptune.create_experiment(name=PARAMS['experiment_name']+'-'+str(PARAMS['target_size']), params=PARAMS):\n",
    "    train_imagenette(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS['learning_rate'] = 1e-6\n",
    "PARAMS['target_size'] = (256,256)\n",
    "PARAMS['BATCH_SIZE'] = 16\n",
    "with neptune.create_experiment(name=PARAMS['experiment_name']+'-'+str(PARAMS['target_size']), params=PARAMS):\n",
    "    train_imagenette(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS['learning_rate'] = 1e-6\n",
    "PARAMS['target_size'] = (384,384)\n",
    "PARAMS['BATCH_SIZE'] = 16\n",
    "with neptune.create_experiment(name=experiment_name+'-'+str(PARAMS['target_size']), params=PARAMS):\n",
    "    train_imagenette(PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Imagenette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, validation_dataset = create_Imagenette_dataset(BATCH_SIZE)\n",
    "# for sample in train_dataset.take(5): # the dataset has to fit in memory with eager iteration\n",
    "#     print(sample[0].shape)\n",
    "#     plot_sample([sample[0]/127.5-1.0], num_res=1)\n",
    "\n",
    "# print([s.shape for s in sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample[0].numpy().min()\n",
    "# sample[0].numpy().max()\n",
    "\n",
    "# print(preprocess_input(sample[0]).numpy().min())\n",
    "# print(preprocess_input(sample[0]).numpy().max())\n",
    "\n",
    "# dataset =  create_mnist_dataset(BATCH_SIZE)\n",
    "# for sample in dataset.take(1): # the dataset has to fit in memory with eager iteration\n",
    "#     plot_sample(sample, num_res=3)\n",
    "\n",
    "# print([s.shape for s in sample])\n",
    "\n",
    "\n",
    "\n",
    "# big_base.summary()\n",
    "\n",
    "# base.summary()\n",
    "\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "# plot_model(big_base, to_file='model.png', show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "# num_params_big = 0\n",
    "# for layer in big_base.layers:\n",
    "#     num_params_big += np.prod(layer.output_shape[1:])\n",
    "# print(num_params_big)\n",
    "\n",
    "# num_params = 0\n",
    "# for layer in base.layers:\n",
    "#     num_params += np.prod(layer.output_shape[1:])\n",
    "# print(num_params)\n",
    "\n",
    "# print(num_params_big - num_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "neptune": {
   "notebookId": "76eda706-149c-46da-91e7-475cb2c4fe0b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

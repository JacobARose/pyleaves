{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jacob/envs/pyleaves2/lib/python3.7/site-packages/scikitplot/plotters.py:37: DeprecationWarning: This module was deprecated in version 0.3.0 and its functions are spread throughout different modules. Please check the documentation and update your function calls as soon as possible. This module will be removed in 0.4.0\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping invalid file log_dir\n",
      "Found previous experiment log_dir__2020-07-23_07-55-50\n",
      "Continuing experiment with start time = 2020-07-23_07-55-50\n",
      "{'BATCH_SIZE': 8,\n",
      " 'METRICS': ['accuracy', 'precision', 'recall'],\n",
      " 'augmentations': [{'flip': 1.0}],\n",
      " 'buffer_size': 400,\n",
      " 'checkpoints_path': '/media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir__2020-07-23_07-55-50/model_dir/checkpoints',\n",
      " 'color_mode': 'grayscale',\n",
      " 'dataset_name': 'Fossil',\n",
      " 'exclude_classes': ['notcataloged',\n",
      "                     'notcatalogued',\n",
      "                     'II. IDs, families uncertain',\n",
      "                     'Unidentified'],\n",
      " 'experiment_dir': '/media/data/jacob/sandbox_logs/Fossil_vgg16',\n",
      " 'experiment_name': 'Fossil_vgg16',\n",
      " 'experiment_start_time': '2020-07-23_07-55-50',\n",
      " 'frozen_layers': None,\n",
      " 'log_dir': '/media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir__2020-07-23_07-55-50',\n",
      " 'loss': 'categorical_crossentropy',\n",
      " 'lr': 1e-05,\n",
      " 'model_dir': '/media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir__2020-07-23_07-55-50/model_dir',\n",
      " 'model_name': 'vgg16',\n",
      " 'neptune_experiment_dir': '/media/data/jacob/sandbox_logs',\n",
      " 'neptune_project_name': 'jacobarose/sandbox',\n",
      " 'num_channels': 3,\n",
      " 'num_epochs': 150,\n",
      " 'optimizer': 'Adam',\n",
      " 'regularization': {'l1': 0.0003},\n",
      " 'saved_model_path': '/media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir__2020-07-23_07-55-50/model_dir/saved_model',\n",
      " 'seed': 45,\n",
      " 'splits': stuf(train=0.5, validation=0.5),\n",
      " 'target_size': (256, 256),\n",
      " 'threshold': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There is a new version of neptune-client 0.4.116 (installed: 0.4.115).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/jacobarose/sandbox/e/SAN-580\n"
     ]
    }
   ],
   "source": [
    "from pyleaves.utils.callback_utils import BackupAndRestore\n",
    "\n",
    "import arrow\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import neptune\n",
    "import os\n",
    "import random\n",
    "from stuf import stuf\n",
    "from pathlib import Path\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '6'\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    print('setting memory growth failed, continuing anyway.')\n",
    "random.seed(84)\n",
    "np.random.seed(58)\n",
    "tf.random.set_seed(34)\n",
    "from tensorflow.keras import backend as K\n",
    "from pyleaves.mains.baseline_train_pipeline import build_model, build_or_restore_model, create_dataset, load_data, prep_dataset, resize_image, ImageLoggerCallback, EarlyStopping, neptune_logger\n",
    "from pyleaves.datasets import leaves_dataset, fossil_dataset, pnas_dataset, base_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "##################################################\n",
    "##################################################\n",
    "def plot_batch(x, y=None, idx = None, encoder=None, rows=4, cols=2, figsize=(15,15)):\n",
    "    assert rows*cols >= len(x), print('Not enough requested rows or columns to plot all images')\n",
    "    \n",
    "    x_norm = (x - x.numpy().min())/(x.numpy().max() - x.numpy().min())\n",
    "    \n",
    "    if encoder is not None:\n",
    "        y = [encoder[i] for i in y.numpy()]\n",
    "    if idx is not None:\n",
    "        idx = idx.numpy()\n",
    "        y = ['-'.join([str(idx[i]),y[i]]) for i in range(idx.shape[0])]\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.ravel()\n",
    "    for i in range(x_norm.shape[0]): #len(axes)):\n",
    "        axes[i].imshow(x_norm[i,...])\n",
    "        if y is not None:\n",
    "            axes[i].set_title(y[i])\n",
    "            \n",
    "    return fig, axes\n",
    "\n",
    "date_format = '%Y-%m-%d_%H-%M-%S'\n",
    "\n",
    "def initialize_experiment(PARAMS, experiment_start_time=None):\n",
    "    PARAMS['experiment_name'] = '_'.join([PARAMS['dataset_name'], PARAMS['model_name']])\n",
    "    PARAMS['experiment_dir'] = os.path.join(PARAMS['neptune_experiment_dir'], PARAMS['experiment_name'])\n",
    "\n",
    "    PARAMS['experiment_start_time'] = experiment_start_time or datetime.now().strftime(date_format)\n",
    "    PARAMS['log_dir'] = os.path.join(PARAMS['experiment_dir'], 'log_dir__'+PARAMS['experiment_start_time'])\n",
    "    PARAMS['model_dir'] = os.path.join(PARAMS['log_dir'],'model_dir')\n",
    "    PARAMS['saved_model_path'] = str(Path(PARAMS['model_dir']) / Path('saved_model'))\n",
    "    PARAMS['checkpoints_path'] = str(Path(PARAMS['model_dir']) / Path('checkpoints'))\n",
    "    \n",
    "def restore_or_initialize_experiment(PARAMS, restore_last=False, prefix='log_dir__', verbose=0):\n",
    "#     date_format = '%Y-%m-%d_%H-%M-%S'\n",
    "    PARAMS = copy.deepcopy(PARAMS)\n",
    "    PARAMS['experiment_name'] = '_'.join([PARAMS['dataset_name'], PARAMS['model_name']])\n",
    "    PARAMS['experiment_dir'] = os.path.join(PARAMS['neptune_experiment_dir'], PARAMS['experiment_name'])\n",
    "\n",
    "    if restore_last:\n",
    "        experiment_files = [(exp_name.split(prefix)[-1], exp_name) for exp_name in os.listdir(PARAMS['experiment_dir'])]\n",
    "        keep_files = []\n",
    "        for i in range(len(experiment_files)):\n",
    "            exp = experiment_files[i]\n",
    "            try:\n",
    "                keep_files.append((datetime.strptime(exp[0], date_format), exp[1]))\n",
    "                if verbose >= 1: print(f'Found previous experiment {exp[1]}')\n",
    "            except ValueError:\n",
    "                if verbose >=2: print(f'skipping invalid file {exp[1]}')\n",
    "                pass\n",
    "        \n",
    "        experiment_files = sorted(keep_files, key= lambda exp: exp[0])\n",
    "        if type(experiment_files)==list and len(experiment_files)>0:\n",
    "            experiment_file = experiment_files[-1]\n",
    "            PARAMS['experiment_start_time'] = experiment_file[0].strftime(date_format)\n",
    "            initialize_experiment(PARAMS, experiment_start_time=PARAMS['experiment_start_time'])\n",
    "            if verbose >= 1: print(f'Continuing experiment with start time =', PARAMS['experiment_start_time'])\n",
    "            return PARAMS\n",
    "        else:\n",
    "            print('No previous experiment in',PARAMS['experiment_dir'], 'with prefix',prefix)\n",
    "\n",
    "    PARAMS['experiment_start_time'] = datetime.now().strftime(date_format)\n",
    "    initialize_experiment(PARAMS, experiment_start_time=PARAMS['experiment_start_time'])\n",
    "    if verbose >= 1: print('Initializing new experiment at time:', PARAMS['experiment_start_time'] )\n",
    "    return PARAMS\n",
    "    \n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "    \n",
    "    \n",
    "PARAMS = {'neptune_project_name':'jacobarose/sandbox',\n",
    "          'neptune_experiment_dir':'/media/data/jacob/sandbox_logs',\n",
    "          'optimizer':'Adam',\n",
    "          'loss':'categorical_crossentropy',\n",
    "          'lr':1e-5,\n",
    "          'color_mode':'grayscale',\n",
    "          'num_channels':3,\n",
    "          'BATCH_SIZE':8,\n",
    "          'buffer_size':400,\n",
    "          'num_epochs':150,\n",
    "          'dataset_name':'Fossil',#'Leaves',#'PNAS',\n",
    "          'threshold':2,\n",
    "          'frozen_layers':None,\n",
    "          'model_name':'vgg16',#'resnet_50_v2',\n",
    "          'splits':{'train':0.5,'validation':0.5},\n",
    "          'seed':45}\n",
    "\n",
    "PARAMS = stuf(PARAMS)\n",
    "\n",
    "PARAMS['exclude_classes'] = ['notcataloged','notcatalogued', 'II. IDs, families uncertain', 'Unidentified']\n",
    "PARAMS['regularization'] = {'l1':3e-4}\n",
    "PARAMS['METRICS'] = ['accuracy','precision','recall']\n",
    "PARAMS['target_size'] = (256,256)#(512,512)#(768,768)#(128,128)#\n",
    "PARAMS['augmentations'] = [{'flip':1.0}]\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "\n",
    "PARAMS = restore_or_initialize_experiment(PARAMS, restore_last=True, prefix='log_dir__', verbose=2)\n",
    "pprint(PARAMS)\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "from pyleaves.utils import ensure_dir_exists\n",
    "\n",
    "neptune.init(project_qualified_name=PARAMS['neptune_project_name'])\n",
    "neptune.create_experiment(name=PARAMS['experiment_name']+'-'+str(dict(PARAMS['splits'])), params=PARAMS)\n",
    "\n",
    "\n",
    "ensure_dir_exists(PARAMS['log_dir'])\n",
    "ensure_dir_exists(PARAMS['model_dir'])\n",
    "neptune.append_tag(PARAMS['dataset_name'])\n",
    "neptune.append_tag(PARAMS['model_name'])\n",
    "neptune.append_tag(str(PARAMS['target_size']))\n",
    "neptune.append_tag(PARAMS['num_channels'])\n",
    "neptune.append_tag(PARAMS['color_mode'])\n",
    "K.clear_session()\n",
    "tf.random.set_seed(PARAMS['seed'])\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "\n",
    "\n",
    "\n",
    "# PARAMS['experiment_name'] = '_'.join([PARAMS['dataset_name'], PARAMS['model_name']])\n",
    "# PARAMS['experiment_dir'] = os.path.join(PARAMS['neptune_experiment_dir'], PARAMS['experiment_name'])\n",
    "# PARAMS['log_dir'] = os.path.join(PARAMS['experiment_dir'], 'log_dir__'+PARAMS['experiment_start_time'])\n",
    "# PARAMS['model_dir'] = os.path.join(PARAMS['log_dir'],'model_dir')\n",
    "# from pathlib import Path\n",
    "# PARAMS['saved_model_path'] = str(Path(PARAMS['model_dir']) / Path('saved_model'))\n",
    "# PARAMS['checkpoints_path'] = str(Path(PARAMS['model_dir']) / Path('checkpoints'))\n",
    "# PARAMS['checkpoints_path'] = str(Path(PARAMS['checkpoints_dir']) / Path(PARAMS['model_name']+'.ckpt-{epoch}'))\n",
    "# pprint(PARAMS)\n",
    "\n",
    "# restore_or_initialize_experiment(PARAMS, restore_last=True, prefix='log_dir_categorical_crossentropy_')#False)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset, validation_dataset, data_files, excluded = create_dataset(dataset_name=PARAMS['dataset_name'],\n",
    "                                                               threshold=PARAMS['threshold'],\n",
    "                                                               batch_size=PARAMS['BATCH_SIZE'],\n",
    "                                                               buffer_size=PARAMS['buffer_size'],\n",
    "                                                               exclude_classes=PARAMS['exclude_classes'],\n",
    "                                                               target_size=PARAMS['target_size'],\n",
    "                                                               num_channels=PARAMS['num_channels'],\n",
    "                                                               color_mode=PARAMS['color_mode'],\n",
    "                                                               splits=PARAMS['splits'],\n",
    "                                                               augmentations=PARAMS['augmentations'],\n",
    "                                                               seed=PARAMS['seed'])\n",
    "\n",
    "PARAMS['num_classes'] = data_files.num_classes\n",
    "PARAMS['splits_size'] = {'train':{},\n",
    "                   'validation':{}}\n",
    "PARAMS['splits_size']['train'] = int(data_files.num_samples*PARAMS['splits']['train'])\n",
    "PARAMS['splits_size']['validation'] = int(data_files.num_samples*PARAMS['splits']['validation'])\n",
    "\n",
    "PARAMS['steps_per_epoch'] = PARAMS['splits_size']['train']//PARAMS['BATCH_SIZE']\n",
    "PARAMS['validation_steps'] = PARAMS['splits_size']['validation']//PARAMS['BATCH_SIZE']\n",
    "\n",
    "neptune.set_property('num_classes',PARAMS['num_classes'])\n",
    "neptune.set_property('steps_per_epoch',PARAMS['steps_per_epoch'])\n",
    "neptune.set_property('validation_steps',PARAMS['validation_steps'])\n",
    "\n",
    "# TODO: log encoder contents as dict\n",
    "encoder = base_dataset.LabelEncoder(data_files.classes)\n",
    "\n",
    "PARAMS['base_learning_rate'] = PARAMS['lr']\n",
    "PARAMS['input_shape'] = (*PARAMS['target_size'],PARAMS['num_channels'])\n",
    "\n",
    "\n",
    "##################################################\n",
    "##################################################\n",
    "# print(len(list(set(data_files.classes))))\n",
    "# print(len(list(set(data_files.classes) - {'notcataloged','notcatalogued'})))\n",
    "# excluded.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fossil:\\n    num_samples: 2435\\n    num_classes: 2\\n    class_count_threshold: 2\\n        '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoder.classes\n",
    "data_files\n",
    "type(excluded)\n",
    "excluded.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import random\n",
    "import os\n",
    "from pprint import pprint\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '5'\n",
    "import tensorflow as tf\n",
    "random.seed(84)\n",
    "np.random.seed(58)\n",
    "tf.random.set_seed(34)\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, TensorBoard, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_datasets as tfds\n",
    "from pyleaves.models import resnet, vgg16\n",
    "from pyleaves.datasets import leaves_dataset, fossil_dataset, pnas_dataset, base_dataset\n",
    "import neptune\n",
    "import arrow\n",
    "from pyleaves.utils import ensure_dir_exists\n",
    "from more_itertools import unzip\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "def image_reshape(x):\n",
    "    return [\n",
    "        tf.image.resize(x, (7, 7)),\n",
    "        tf.image.resize(x, (14, 14)),\n",
    "        x\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_img(image_path):#, img_size=(224,224)):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return img\n",
    "\n",
    "\n",
    "def resize(image, h=512, w=512):\n",
    "    return tf.image.resize_with_pad(image, target_height=h, target_width=w)\n",
    "\n",
    "\n",
    "def rgb2gray_3channel(img, label):\n",
    "    '''\n",
    "    Convert rgb image to grayscale, but keep num_channels=3\n",
    "    '''\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "    img = tf.image.grayscale_to_rgb(img)\n",
    "    return img, label\n",
    "\n",
    "def rgb2gray_1channel(img, label):\n",
    "    '''\n",
    "    Convert rgb image to grayscale, num_channels from 3 to 1\n",
    "    '''\n",
    "    img = tf.image.rgb_to_grayscale(img)\n",
    "    return img, label\n",
    "\n",
    "\n",
    "def _cond_apply(x, y, func, prob):\n",
    "    \"\"\"Conditionally apply func to x and y with probability prob.\n",
    "    \"\"\"\n",
    "    return tf.cond((tf.random.uniform([], 0, 1) >= (1.0 - prob)), lambda: func(x,y), lambda: (x,y))\n",
    "\n",
    "\n",
    "def augment_sample(x, y):\n",
    "    x = tf.image.random_flip_left_right(x, seed=2)\n",
    "    x = tf.image.random_flip_up_down(x, seed=2)\n",
    "    return x, y\n",
    "                \n",
    "\n",
    "preprocess_input(tf.zeros([4, 224, 224, 3]))\n",
    "def apply_preprocess(x, y, num_classes=10):\n",
    "    return preprocess_input(x), tf.one_hot(y, depth=num_classes)\n",
    "    \n",
    "    \n",
    "\n",
    "def create_mnist_dataset(batch_size):\n",
    "    (train_images, _), (_, _) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "    train_images = train_images.reshape([-1, 28, 28, 1]).astype('float32')\n",
    "    train_images = train_images/127.5  - 1\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(train_images)\n",
    "    dataset = dataset.map(image_reshape)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.shuffle(len(train_images))\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prep_dataset(dataset,\n",
    "                 batch_size=32, \n",
    "                 buffer_size=100, \n",
    "                 shuffle=False,\n",
    "                 target_size=(512,512),\n",
    "                 num_channels=3,\n",
    "                 color_mode='grayscale',\n",
    "                 num_classes=10,\n",
    "                 augment=False, \n",
    "                 aug_prob=1.0):\n",
    "    dataset = dataset.map(lambda x,y: (resize(x, *target_size),y), num_parallel_calls=-1)\n",
    "    \n",
    "    dataset = dataset.map(lambda x,y: apply_preprocess(x,y,num_classes),num_parallel_calls=-1)\n",
    "    \n",
    "    if color_mode=='grayscale':\n",
    "        if num_channels==3:\n",
    "            dataset = dataset.map(lambda x,y: rgb2gray_3channel(x, y), num_parallel_calls=-1)\n",
    "        elif num_channels==1:\n",
    "            dataset = dataset.map(lambda x,y: rgb2gray_1channel(x, y), num_parallel_calls=-1)\n",
    "    \n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    if augment:\n",
    "        dataset = dataset.map(lambda x,y: _cond_apply(x, y, augment_sample, prob=aug_prob), num_parallel_calls=-1)\n",
    "#         dataset = dataset.map(augment_sample, num_parallel_calls=-1)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_Imagenette_dataset(batch_size,\n",
    "                              target_size=(512,512), \n",
    "                              augment_train=True,\n",
    "                              aug_prob=1.0):\n",
    "    \n",
    "    data, info = tfds.load('Imagenette', as_supervised=True, with_info=True)\n",
    "    train_data = prep_dataset(data['train'], \n",
    "                              batch_size=batch_size, \n",
    "                              buffer_size=info.splits['train'].num_examples,\n",
    "                              shuffle=True,\n",
    "                              target_size=target_size,\n",
    "                              augment=augment_train,\n",
    "                              aug_prob=aug_prob)\n",
    "    val_data = prep_dataset(data['validation'], \n",
    "                            batch_size=batch_size,\n",
    "                            target_size=target_size)\n",
    "    \n",
    "    return train_data, val_data, info\n",
    "\n",
    "\n",
    "\n",
    "def partition_data(data, partitions=OrderedDict({'train':0.5,'test':0.5})):\n",
    "    '''\n",
    "    Split data into named partitions by fraction\n",
    "\n",
    "    Example:\n",
    "    --------\n",
    "    >> split_data = partition_data(data, partitions=OrderedDict({'train':0.4,'val':0.1,'test':0.5}))\n",
    "    '''\n",
    "    num_rows = len(data)\n",
    "    output={}\n",
    "    taken = 0.0\n",
    "    for k,v in partitions.items():\n",
    "        idx = (int(taken*num_rows),int((taken+v)*num_rows))\n",
    "        output.update({k:data[idx[0]:idx[1]]})\n",
    "        taken+=v\n",
    "    assert taken <= 1.0\n",
    "    return output\n",
    "\n",
    "def load_data(dataset_name='PNAS', splits={'train':0.7,'validation':0.3}, threshold=50):\n",
    "    datasets = {\n",
    "            'PNAS': pnas_dataset.PNASDataset(),\n",
    "            'Leaves': leaves_dataset.LeavesDataset(),\n",
    "            'Fossil': fossil_dataset.FossilDataset()\n",
    "            }\n",
    "    data_files = datasets[dataset_name]\n",
    "    \n",
    "    data_files.exclude_rare_classes(threshold=threshold)\n",
    "    encoder = base_dataset.LabelEncoder(data_files.classes)\n",
    "    data_files, _ = data_files.enforce_class_whitelist(class_names=encoder.classes)\n",
    "\n",
    "    x = list(data_files.data['path'].values)\n",
    "    y = np.array(encoder.encode(data_files.data['family']))\n",
    "\n",
    "    shuffled_data = list(zip(x,y))\n",
    "    random.shuffle(shuffled_data)\n",
    "    partitioned_data = partition_data(data=shuffled_data,\n",
    "                                      partitions=OrderedDict(splits))\n",
    "    split_data = {k:v for k,v in partitioned_data.items() if len(v)>0}\n",
    "    \n",
    "    for subset, subset_data in split_data.items():\n",
    "        split_data[subset] = [list(i) for i in unzip(subset_data)]\n",
    "    \n",
    "    paths = tf.data.Dataset.from_tensor_slices(split_data['train'][0])\n",
    "    labels = tf.data.Dataset.from_tensor_slices(split_data['train'][1])\n",
    "    train_data = tf.data.Dataset.zip((paths, labels))\n",
    "    train_data = train_data.map(lambda x,y: (tf.image.convert_image_dtype(load_img(x)*255.0,dtype=tf.uint8),y), num_parallel_calls=-1)\n",
    "    \n",
    "    paths = tf.data.Dataset.from_tensor_slices(split_data['validation'][0])\n",
    "    labels = tf.data.Dataset.from_tensor_slices(split_data['validation'][1])\n",
    "    validation_data = tf.data.Dataset.zip((paths, labels))\n",
    "    validation_data = validation_data.map(lambda x,y: (tf.image.convert_image_dtype(load_img(x)*255.0,dtype=tf.uint8),y), num_parallel_calls=-1)\n",
    "    \n",
    "    return {'train':train_data, \n",
    "            'validation':validation_data}, data_files\n",
    "\n",
    "\n",
    "def create_dataset(dataset_name='PNAS',\n",
    "                   batch_size=32,\n",
    "                   target_size=(512,512),\n",
    "                   num_channels=1,\n",
    "                   color_mode='grayscale',\n",
    "                   splits={'train':0.7,'validation':0.3},\n",
    "                   augment_train=True,\n",
    "                   aug_prob=1.0):\n",
    "    \n",
    "    dataset, data_files = load_data(dataset_name=dataset_name, splits=splits)\n",
    "    train_data = prep_dataset(dataset['train'], \n",
    "                              batch_size=batch_size, \n",
    "                              buffer_size=int(data_files.num_samples*splits['train']),\n",
    "                              shuffle=True,\n",
    "                              target_size=target_size,\n",
    "                              num_channels=num_channels,\n",
    "                              color_mode=color_mode,\n",
    "                              num_classes=data_files.num_classes,\n",
    "                              augment=augment_train,\n",
    "                              aug_prob=aug_prob)\n",
    "    val_data = prep_dataset(dataset['validation'], \n",
    "                            batch_size=batch_size,\n",
    "                            target_size=target_size,\n",
    "                            num_channels=num_channels,\n",
    "                            color_mode=color_mode,\n",
    "                            num_classes=data_files.num_classes)\n",
    "    return train_data, val_data, data_files\n",
    "\n",
    "\n",
    "# def create_pnas_dataset(batch_size):\n",
    "#     data_files = pnas_dataset.PNASDataset()\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices(data_files.data['path'])\n",
    "#     dataset = dataset.map(load_img, num_parallel_calls=-1)\n",
    "#     dataset = dataset.map(resize, num_parallel_calls=-1)\n",
    "#     dataset = dataset.cache()\n",
    "#     dataset = dataset.shuffle(20)#len(data_files.data))\n",
    "#     dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "#     dataset = dataset.prefetch(1)\n",
    "#     return dataset\n",
    "\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "    \n",
    "\n",
    "def build_base_vgg16_RGB(PARAMS):\n",
    "#     if PARAMS['optimizer']=='Adam':\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=PARAMS['lr'])\n",
    "    \n",
    "    base = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                             include_top=False,\n",
    "                                             input_tensor=Input(shape=(*PARAMS['target_size'],3)))\n",
    "\n",
    "    return base\n",
    "\n",
    "\n",
    "def build_head(base, num_classes=10):\n",
    "    global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "    dense1 = tf.keras.layers.Dense(2048,activation='relu',name='dense1')\n",
    "    dense2 = tf.keras.layers.Dense(512,activation='relu',name='dense2')\n",
    "    prediction_layer = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    model = tf.keras.Sequential([\n",
    "        base,\n",
    "        global_average_layer,dense1,dense2,\n",
    "        prediction_layer\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def build_model(PARAMS):\n",
    "    '''\n",
    "    model_params = {\n",
    "                'num_classes':PARAMS['num_classes'],\n",
    "                'frozen_layers':PARAMS['frozen_layers'],\n",
    "                'input_shape':(*PARAMS['target_size'],PARAMS['num_channels']),\n",
    "                'base_learning_rate':PARAMS['lr'],\n",
    "                'regularization':PARAMS['regularization'],\n",
    "                'loss':'categorical_crossentropy'.\n",
    "                'METRICS':['accuracy']\n",
    "                }\n",
    "    '''\n",
    "    \n",
    "    if PARAMS['model_name']=='vgg16':\n",
    "        if PARAMS['num_channels']==1:\n",
    "            model_builder = vgg16.VGG16GrayScale(PARAMS)\n",
    "            build_base = model_builder.build_base\n",
    "        else:\n",
    "            build_base = partial(build_base_vgg16_RGB, PARAMS=PARAMS)\n",
    "            \n",
    "    elif PARAMS['model_name'].startswith('resnet'):\n",
    "        model_builder = resnet.ResNet(PARAMS)\n",
    "        build_base = model_builder.build_base\n",
    "\n",
    "    base = build_base()\n",
    "    model = build_head(base, num_classes=PARAMS['num_classes'])\n",
    "\n",
    "    if PARAMS['optimizer']=='Adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=PARAMS['lr'])\n",
    "        \n",
    "    if PARAMS['loss']=='categorical_crossentropy':\n",
    "        loss = 'categorical_crossentropy'\n",
    "        \n",
    "    if 'accuracy' in PARAMS['METRICS']:\n",
    "        METRICS = ['accuracy']\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=METRICS)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_data(logs):\n",
    "    for k, v in logs.items():\n",
    "        neptune.log_metric(k, v)\n",
    "\n",
    "        \n",
    "##########################################################################\n",
    "##########################################################################\n",
    "\n",
    "        \n",
    "def plot_sample(sample, num_res=1):\n",
    "    num_samples = min(64, len(sample[0]))\n",
    "\n",
    "    grid = gridspec.GridSpec(num_res, num_samples)\n",
    "    grid.update(left=0, bottom=0, top=1, right=1, wspace=0.01, hspace=0.01)\n",
    "    fig = plt.figure(figsize=[num_samples, num_res])\n",
    "    for x in range(num_res):\n",
    "        images = sample[x].numpy() #this converts the tensor to a numpy array\n",
    "        images = np.squeeze(images)\n",
    "        for y in range(num_samples):\n",
    "            ax = fig.add_subplot(grid[x, y])\n",
    "            ax.set_axis_off()\n",
    "            ax.imshow((images[y] + 1.0)/2, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# neptune_logger = tf.keras.callbacks.LambdaCallback(on_epoch_end=lambda epoch, logs: log_data(logs))\n",
    "neptune_logger = tf.keras.callbacks.LambdaCallback(on_batch_end=lambda batch, logs: log_data(logs),\n",
    "                                                  on_epoch_end=lambda epoch, logs: log_data(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyleaves.utils.neptune_utils import ImageLoggerCallback\n",
    "\n",
    "def train_imagenette(PARAMS):\n",
    "\n",
    "    neptune.append_tag(PARAMS['dataset_name'])\n",
    "    neptune.append_tag(PARAMS['model_name'])\n",
    "    \n",
    "    K.clear_session()\n",
    "    tf.random.set_seed(34)\n",
    "    target_size = PARAMS['target_size']\n",
    "    BATCH_SIZE = PARAMS['BATCH_SIZE']\n",
    "    \n",
    "    train_dataset, validation_dataset, info = create_Imagenette_dataset(BATCH_SIZE, target_size=target_size, augment_train=PARAMS['augment_train'])\n",
    "    num_classes = info.features['label'].num_classes\n",
    "    \n",
    "    encoder = base_dataset.LabelEncoder(info.features['label'].names)\n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x,y: apply_preprocess(x,y,num_classes),num_parallel_calls=-1)\n",
    "    validation_dataset = validation_dataset.map(lambda x,y: apply_preprocess(x,y,num_classes),num_parallel_calls=-1)\n",
    "    \n",
    "    PARAMS['num_classes'] = num_classes\n",
    "    steps_per_epoch = info.splits['train'].num_examples//BATCH_SIZE\n",
    "    validation_steps = info.splits['validation'].num_examples//BATCH_SIZE\n",
    "\n",
    "    neptune.set_property('num_classes',num_classes)\n",
    "    neptune.set_property('steps_per_epoch',steps_per_epoch)\n",
    "    neptune.set_property('validation_steps',validation_steps)\n",
    "\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=PARAMS['learning_rate'])\n",
    "    loss = 'categorical_crossentropy'\n",
    "    METRICS = ['accuracy']\n",
    "\n",
    "    base = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "                                             include_top=False,\n",
    "                                             input_tensor=Input(shape=(*target_size,3)))\n",
    "\n",
    "    # TODO try freezing weights for input_shape != (224,224)\n",
    "\n",
    "    model = build_head(base, num_classes=num_classes)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=METRICS)\n",
    "\n",
    "    callbacks = [neptune_logger,\n",
    "                 ImageLoggerCallback(data=train_dataset, freq=10, max_images=-1, name='train', encoder=encoder),\n",
    "                 ImageLoggerCallback(data=validation_dataset, freq=10, max_images=-1, name='val', encoder=encoder),\n",
    "                 EarlyStopping(monitor='val_loss', patience=2, verbose=1)]\n",
    "\n",
    "    model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "    pprint(PARAMS)\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=10,\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=validation_dataset,\n",
    "                        shuffle=True,\n",
    "                        initial_epoch=0,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps)\n",
    "    \n",
    "    \n",
    "def train_pnas(PARAMS):\n",
    "    ensure_dir_exists(PARAMS['log_dir'])\n",
    "    ensure_dir_exists(PARAMS['model_dir'])\n",
    "    neptune.append_tag(PARAMS['dataset_name'])\n",
    "    neptune.append_tag(PARAMS['model_name'])\n",
    "    neptune.append_tag(str(PARAMS['target_size']))\n",
    "    neptune.append_tag(PARAMS['num_channels'])\n",
    "    neptune.append_tag(PARAMS['color_mode'])\n",
    "    \n",
    "    K.clear_session()\n",
    "    tf.random.set_seed(34)\n",
    "    \n",
    "    \n",
    "    train_dataset, validation_dataset, data_files = create_dataset(dataset_name=PARAMS['dataset_name'],\n",
    "                                                                   batch_size=PARAMS['BATCH_SIZE'],\n",
    "                                                                   target_size=PARAMS['target_size'],\n",
    "                                                                   num_channels=PARAMS['num_channels'],\n",
    "                                                                   color_mode=PARAMS['color_mode'],\n",
    "                                                                   splits=PARAMS['splits'],\n",
    "                                                                   augment_train=PARAMS['augment_train'],\n",
    "                                                                   aug_prob=PARAMS['aug_prob'])\n",
    "    \n",
    "    \n",
    "    PARAMS['num_classes'] = data_files.num_classes\n",
    "    PARAMS['splits_size'] = {'train':{},\n",
    "                       'validation':{}}\n",
    "    PARAMS['splits_size']['train'] = data_files.num_samples*PARAMS['splits']['train']\n",
    "    PARAMS['splits_size']['validation'] = data_files.num_samples*PARAMS['splits']['validation']\n",
    "\n",
    "    steps_per_epoch = PARAMS['splits_size']['train']//PARAMS['BATCH_SIZE']\n",
    "    validation_steps = PARAMS['splits_size']['validation']//PARAMS['BATCH_SIZE']\n",
    "   \n",
    "    neptune.set_property('num_classes',PARAMS['num_classes'])\n",
    "    neptune.set_property('steps_per_epoch',steps_per_epoch)\n",
    "    neptune.set_property('validation_steps',validation_steps)\n",
    "    \n",
    "    \n",
    "    encoder = base_dataset.LabelEncoder(data_files.classes)\n",
    "#     train_dataset = train_dataset.map(lambda x,y: apply_preprocess(x,y,PARAMS['num_classes']),num_parallel_calls=-1)\n",
    "#     validation_dataset = validation_dataset.map(lambda x,y: apply_preprocess(x,y,PARAMS['num_classes']),num_parallel_calls=-1)\n",
    "\n",
    "    \n",
    "#     METRICS = ['accuracy']\n",
    "    callbacks = [neptune_logger,\n",
    "                 ImageLoggerCallback(data=train_dataset, freq=10, max_images=-1, name='train', encoder=encoder),\n",
    "                 ImageLoggerCallback(data=validation_dataset, freq=10, max_images=-1, name='val', encoder=encoder),\n",
    "                 EarlyStopping(monitor='val_loss', patience=30, verbose=1)]\n",
    "    \n",
    "    PARAMS['base_learning_rate'] = PARAMS['lr']\n",
    "    PARAMS['input_shape'] = (*PARAMS['target_size'],PARAMS['num_channels'])\n",
    "    model = build_model(PARAMS)\n",
    "    \n",
    "    \n",
    "#     if PARAMS['optimizer']=='Adam':\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=PARAMS['lr'])\n",
    "    \n",
    "#     base = tf.keras.applications.vgg16.VGG16(weights='imagenet',\n",
    "#                                              include_top=False,\n",
    "#                                              input_tensor=Input(shape=(*PARAMS['target_size'],3)))\n",
    "\n",
    "#     model = build_head(base, num_classes=PARAMS['num_classes'])\n",
    "\n",
    "#     model.compile(optimizer=optimizer,\n",
    "#                   loss=PARAMS['loss'],\n",
    "#                   metrics=METRICS)\n",
    "    \n",
    "    model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "    pprint(PARAMS)\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=PARAMS['num_epochs'],\n",
    "                        callbacks=callbacks,\n",
    "                        validation_data=validation_dataset,\n",
    "                        shuffle=True,\n",
    "                        initial_epoch=0,\n",
    "                        steps_per_epoch=steps_per_epoch,\n",
    "                        validation_steps=validation_steps)\n",
    "    \n",
    "    \n",
    "    for k,v in PARAMS.items():\n",
    "        neptune.set_property(str(k),str(v))\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There is a new version of neptune-client 0.4.116 (installed: 0.4.115).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/jacobarose/sandbox/e/SAN-463\n",
      "INFO:tensorflow:Assets written to: /media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir/categorical_crossentropy/2020-07-02_09-13-00/model_dir/vgg16_grayscale-saved_base_model/assets\n",
      "Saved model vgg16_grayscale at path: /media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir/categorical_crossentropy/2020-07-02_09-13-00/model_dir/vgg16_grayscale-saved_base_model\n",
      "{'BATCH_SIZE': 48,\n",
      " 'METRICS': ('accuracy',),\n",
      " 'augment_train': True,\n",
      " 'base_learning_rate': 1e-05,\n",
      " 'color_mode': 'grayscale',\n",
      " 'config_filepath': '/media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir/categorical_crossentropy/2020-07-02_09-13-00/model_dir/vgg16_grayscale-model_config.json',\n",
      " 'dataset_name': 'Fossil',\n",
      " 'experiment_dir': '/media/data/jacob/sandbox_logs',\n",
      " 'experiment_name': 'Fossil_vgg16',\n",
      " 'experiment_start_time': '2020-07-02_09-13-00',\n",
      " 'frozen_layers': None,\n",
      " 'input_shape': (224, 224, 1),\n",
      " 'log_dir': '/media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir/categorical_crossentropy/2020-07-02_09-13-00',\n",
      " 'loss': 'categorical_crossentropy',\n",
      " 'lr': 1e-05,\n",
      " 'model_dir': '/media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir/categorical_crossentropy/2020-07-02_09-13-00/model_dir',\n",
      " 'model_name': 'vgg16',\n",
      " 'neptune_project_name': 'jacobarose/sandbox',\n",
      " 'num_channels': 1,\n",
      " 'num_classes': 14,\n",
      " 'num_epochs': 150,\n",
      " 'optimizer': 'Adam',\n",
      " 'regularization': {'l1': 0.001},\n",
      " 'splits': stuf(train=0.5, validation=0.5),\n",
      " 'splits_size': {'train': 2972.5, 'validation': 2972.5},\n",
      " 'target_size': (224, 224),\n",
      " 'weights_filepath': '/media/data/jacob/sandbox_logs/Fossil_vgg16/log_dir/categorical_crossentropy/2020-07-02_09-13-00/model_dir/vgg16_grayscale-model_weights.h5'}\n",
      "Train for 61.0 steps, validate for 61.0 steps\n",
      "Epoch 1/150\n",
      "Batch 1: Logged 48 train images to neptune\n",
      "Batch 1: Logged 48 val images to neptune\n",
      "10/61 [===>..........................] - ETA: 14:12 - loss: 3.3682 - accuracy: 0.1583Batch 11: Logged 48 train images to neptune\n",
      "Batch 11: Logged 48 val images to neptune\n",
      "20/61 [========>.....................] - ETA: 6:39 - loss: 2.8528 - accuracy: 0.2021Batch 21: Logged 48 train images to neptune\n",
      "Batch 21: Logged 48 val images to neptune\n",
      "30/61 [=============>................] - ETA: 3:48 - loss: 2.6234 - accuracy: 0.2264Batch 31: Logged 48 train images to neptune\n",
      "Batch 31: Logged 48 val images to neptune\n",
      "40/61 [==================>...........] - ETA: 2:11 - loss: 2.5094 - accuracy: 0.2380Batch 41: Logged 48 train images to neptune\n",
      "Batch 41: Logged 48 val images to neptune\n",
      "50/61 [=======================>......] - ETA: 1:01 - loss: 2.4264 - accuracy: 0.2492Batch 51: Logged 48 train images to neptune\n",
      "Batch 51: Logged 48 val images to neptune\n",
      "60/61 [============================>.] - ETA: 5s - loss: 2.3745 - accuracy: 0.2590 Batch 61: Logged 48 train images to neptune\n",
      "Batch 61: Logged 48 val images to neptune\n",
      "61/61 [==============================] - 334s 5s/step - loss: 2.3742 - accuracy: 0.2589 - val_loss: 2.1046 - val_accuracy: 0.2913\n",
      "Epoch 2/150\n",
      "61/61 [==============================] - 28s 465ms/step - loss: 2.0729 - accuracy: 0.3057 - val_loss: 2.0451 - val_accuracy: 0.3190\n",
      "Epoch 3/150\n",
      "61/61 [==============================] - 29s 475ms/step - loss: 1.9526 - accuracy: 0.3402 - val_loss: 2.0391 - val_accuracy: 0.3125\n",
      "Epoch 4/150\n",
      "61/61 [==============================] - 29s 482ms/step - loss: 1.8652 - accuracy: 0.3692 - val_loss: 1.9445 - val_accuracy: 0.3463\n",
      "Epoch 5/150\n",
      "61/61 [==============================] - 29s 482ms/step - loss: 1.7488 - accuracy: 0.4300 - val_loss: 1.9463 - val_accuracy: 0.3630\n",
      "Epoch 6/150\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 1.6555 - accuracy: 0.4522 - val_loss: 1.9096 - val_accuracy: 0.3689\n",
      "Epoch 7/150\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 1.5673 - accuracy: 0.4805 - val_loss: 1.9102 - val_accuracy: 0.3589\n",
      "Epoch 8/150\n",
      "61/61 [==============================] - 29s 482ms/step - loss: 1.4528 - accuracy: 0.5266 - val_loss: 1.9030 - val_accuracy: 0.3671\n",
      "Epoch 9/150\n",
      "61/61 [==============================] - 29s 483ms/step - loss: 1.3108 - accuracy: 0.5686 - val_loss: 1.9533 - val_accuracy: 0.3654\n",
      "Epoch 10/150\n",
      "61/61 [==============================] - 29s 479ms/step - loss: 1.2247 - accuracy: 0.6107 - val_loss: 1.9887 - val_accuracy: 0.3535\n",
      "Epoch 11/150\n",
      "61/61 [==============================] - 29s 476ms/step - loss: 1.1345 - accuracy: 0.6308 - val_loss: 2.1342 - val_accuracy: 0.3412\n",
      "Epoch 12/150\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 1.0288 - accuracy: 0.6742 - val_loss: 2.0326 - val_accuracy: 0.3709\n",
      "Epoch 13/150\n",
      "61/61 [==============================] - 29s 478ms/step - loss: 0.8959 - accuracy: 0.7121 - val_loss: 2.1420 - val_accuracy: 0.3829\n",
      "Epoch 14/150\n",
      "61/61 [==============================] - 29s 478ms/step - loss: 0.7662 - accuracy: 0.7671 - val_loss: 2.2273 - val_accuracy: 0.3760\n",
      "Epoch 15/150\n",
      "61/61 [==============================] - 29s 477ms/step - loss: 0.7116 - accuracy: 0.7787 - val_loss: 2.2518 - val_accuracy: 0.3767\n",
      "Epoch 16/150\n",
      "61/61 [==============================] - 29s 480ms/step - loss: 0.6602 - accuracy: 0.7958 - val_loss: 2.3492 - val_accuracy: 0.3801\n",
      "Epoch 17/150\n",
      "61/61 [==============================] - 29s 479ms/step - loss: 0.5574 - accuracy: 0.8330 - val_loss: 2.4188 - val_accuracy: 0.3774\n",
      "Epoch 18/150\n",
      "61/61 [==============================] - 29s 478ms/step - loss: 0.4382 - accuracy: 0.8648 - val_loss: 2.6445 - val_accuracy: 0.3545\n",
      "Epoch 19/150\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 0.3864 - accuracy: 0.8839 - val_loss: 2.6314 - val_accuracy: 0.3835\n",
      "Epoch 20/150\n",
      "61/61 [==============================] - 29s 483ms/step - loss: 0.3318 - accuracy: 0.9092 - val_loss: 2.8466 - val_accuracy: 0.3706\n",
      "Epoch 21/150\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 0.2943 - accuracy: 0.9184 - val_loss: 2.9059 - val_accuracy: 0.3648\n",
      "Epoch 22/150\n",
      "61/61 [==============================] - 29s 477ms/step - loss: 0.2832 - accuracy: 0.9146 - val_loss: 2.9055 - val_accuracy: 0.3730\n",
      "Epoch 23/150\n",
      "61/61 [==============================] - 29s 476ms/step - loss: 0.1992 - accuracy: 0.9433 - val_loss: 3.0427 - val_accuracy: 0.3706\n",
      "Epoch 24/150\n",
      "61/61 [==============================] - 29s 479ms/step - loss: 0.1654 - accuracy: 0.9583 - val_loss: 3.1467 - val_accuracy: 0.3719\n",
      "Epoch 25/150\n",
      "61/61 [==============================] - 29s 478ms/step - loss: 0.1403 - accuracy: 0.9638 - val_loss: 3.3523 - val_accuracy: 0.3733\n",
      "Epoch 26/150\n",
      "61/61 [==============================] - 29s 479ms/step - loss: 0.1205 - accuracy: 0.9710 - val_loss: 3.3845 - val_accuracy: 0.3730\n",
      "Epoch 27/150\n",
      "61/61 [==============================] - 29s 483ms/step - loss: 0.0840 - accuracy: 0.9829 - val_loss: 3.5677 - val_accuracy: 0.3794\n",
      "Epoch 28/150\n",
      "61/61 [==============================] - 29s 478ms/step - loss: 0.0619 - accuracy: 0.9860 - val_loss: 3.6190 - val_accuracy: 0.3648\n",
      "Epoch 29/150\n",
      "61/61 [==============================] - 29s 478ms/step - loss: 0.0669 - accuracy: 0.9867 - val_loss: 3.8996 - val_accuracy: 0.3808\n",
      "Epoch 30/150\n",
      "61/61 [==============================] - 29s 479ms/step - loss: 0.0856 - accuracy: 0.9775 - val_loss: 3.8413 - val_accuracy: 0.3805\n",
      "Epoch 31/150\n",
      "61/61 [==============================] - 29s 479ms/step - loss: 0.0896 - accuracy: 0.9792 - val_loss: 3.7762 - val_accuracy: 0.3815\n",
      "Epoch 32/150\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 0.0537 - accuracy: 0.9887 - val_loss: 3.9045 - val_accuracy: 0.3702\n",
      "Epoch 33/150\n",
      "61/61 [==============================] - 29s 475ms/step - loss: 0.0415 - accuracy: 0.9918 - val_loss: 4.0764 - val_accuracy: 0.3883\n",
      "Epoch 34/150\n",
      "61/61 [==============================] - 29s 483ms/step - loss: 0.0287 - accuracy: 0.9962 - val_loss: 4.2037 - val_accuracy: 0.3808\n",
      "Epoch 35/150\n",
      "61/61 [==============================] - 29s 478ms/step - loss: 0.0203 - accuracy: 0.9976 - val_loss: 4.2844 - val_accuracy: 0.3835\n",
      "Epoch 36/150\n",
      "61/61 [==============================] - 29s 476ms/step - loss: 0.0146 - accuracy: 0.9986 - val_loss: 4.3546 - val_accuracy: 0.3682\n",
      "Epoch 37/150\n",
      "61/61 [==============================] - 29s 480ms/step - loss: 0.0090 - accuracy: 0.9997 - val_loss: 4.4004 - val_accuracy: 0.3805\n",
      "Epoch 38/150\n",
      "61/61 [==============================] - 29s 481ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 4.4926 - val_accuracy: 0.3849\n",
      "Epoch 00038: early stopping\n"
     ]
    }
   ],
   "source": [
    "# PARAMS = {'neptune_project_name':'jacobarose/sandbox',\n",
    "#           'experiment_name':'pnas_test',\n",
    "#           'experiment_dir':'/media/data/jacob/sandbox_logs',\n",
    "#           'experiment_start_time':arrow.utcnow().format('YYYY-MM-DD_HH-mm-ss'),\n",
    "#           'optimizer':'Adam',\n",
    "#           'loss':'categorical_crossentropy',\n",
    "#           'lr':1e-5,\n",
    "#           'target_size':(224,224), #(256,256),\n",
    "#           'BATCH_SIZE':48,\n",
    "#           'num_epochs':150,\n",
    "#           'dataset_name':'pnas',\n",
    "#           'model_name':'vgg16',\n",
    "#           'augment_train':True,\n",
    "#           'splits':{'train':0.5,'validation':0.5}}\n",
    "\n",
    "# PARAMS = {'neptune_project_name':'jacobarose/sandbox',\n",
    "#           'experiment_name':'fossil_test',\n",
    "#           'experiment_dir':'/media/data/jacob/sandbox_logs',\n",
    "#           'experiment_start_time':arrow.utcnow().format('YYYY-MM-DD_HH-mm-ss'),\n",
    "#           'optimizer':'Adam',\n",
    "#           'loss':'categorical_crossentropy',\n",
    "#           'lr':1e-5,\n",
    "#           'METRICS':['accuracy'],\n",
    "#           'target_size':(224,224),\n",
    "#           'num_channels':1,\n",
    "#           'BATCH_SIZE':48,\n",
    "#           'num_epochs':150,\n",
    "#           'dataset_name':'Fossil',\n",
    "#           'model_name':'vgg16',\n",
    "#           'augment_train':True,\n",
    "#           'splits':{'train':0.5,'validation':0.5}}\n",
    "\n",
    "\n",
    "from stuf import stuf\n",
    "\n",
    "\n",
    "PARAMS = {'neptune_project_name':'jacobarose/sandbox',\n",
    "          'experiment_dir':'/media/data/jacob/sandbox_logs',\n",
    "          'experiment_start_time':arrow.utcnow().format('YYYY-MM-DD_HH-mm-ss'),\n",
    "          'optimizer':'Adam',\n",
    "          'loss':'categorical_crossentropy',\n",
    "          'lr':1e-5,\n",
    "#           'METRICS':'accuracy', #['accuracy'],\n",
    "#           'target_size':(224,224),\n",
    "          'color_mode':'grayscale',\n",
    "          'num_channels':1,\n",
    "          'BATCH_SIZE':48,\n",
    "          'num_epochs':150,\n",
    "          'dataset_name':'Fossil',\n",
    "          'frozen_layers':None,\n",
    "          'model_name':'vgg16',\n",
    "          'augment_train':True,\n",
    "          'aug_prob':0.5,\n",
    "          'splits':stuf({'train':0.5,'validation':0.5})}\n",
    "\n",
    "PARAMS = stuf(PARAMS)\n",
    "\n",
    "PARAMS['experiment_name'] = '_'.join([PARAMS['dataset_name'], PARAMS['model_name']])\n",
    "PARAMS['regularization'] = {'l1':1e-3}\n",
    "PARAMS['METRICS'] = 'accuracy',\n",
    "PARAMS['target_size'] = (224,224)\n",
    "\n",
    "\n",
    "PARAMS['log_dir'] = os.path.join(PARAMS['experiment_dir'], PARAMS['experiment_name'], 'log_dir', PARAMS['loss'], PARAMS['experiment_start_time'])\n",
    "PARAMS['model_dir'] = os.path.join(PARAMS['log_dir'],'model_dir')\n",
    "\n",
    "neptune.init(project_qualified_name=PARAMS['neptune_project_name'])\n",
    "with neptune.create_experiment(name=PARAMS['experiment_name']+'-'+str(PARAMS['splits']), params=PARAMS):\n",
    "    train_pnas(PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "neptune": {
   "notebookId": "97f44883-1330-46cd-abdf-faf0620f86f7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

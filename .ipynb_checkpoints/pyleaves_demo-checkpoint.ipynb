{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, navigate to your chosen working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd ~/..\n",
    "# %pwd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:88:00.0, compute capability: 6.1\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n",
      "['/device:GPU:0']\n",
      "OpenCV is built with OpenMP support. This usually results in poor performance. For details, see https://github.com/tensorpack/benchmarks/blob/master/ImageNet/benchmark-opencv-resize.py\n",
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "random_seed = 34\n",
    "import random\n",
    "random.seed(random_seed)\n",
    "import dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stuf import stuf\n",
    "\n",
    "import os\n",
    "gpu=5\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"#str(gpu)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf;\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf_config=tf.ConfigProto(log_device_placement=True)\n",
    "tf_config.gpu_options.allocator_type = 'BFC'\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.allow_soft_placement = True\n",
    "sess = tf.Session(config=tf_config)\n",
    "K.set_session(sess)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "print(get_available_gpus())\n",
    "\n",
    "import pyleaves\n",
    "\n",
    "from pyleaves.analysis.img_utils import convert_to_png\n",
    "from pyleaves import leavesdb\n",
    "from pyleaves.data_pipeline.tensorpack_loaders import get_multiprocess_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"Fossil\"\n",
    "# tf.test.gpu_device_name()\n",
    "# dir(tf.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.** Initialize and connect to database in local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with sql db at location /home/jacob/scripts/leavesdb.db\n",
      "/home/jacob/scripts/leavesdb.db\n"
     ]
    }
   ],
   "source": [
    "# local_db = os.path.join(os.getcwd(),'pyleaves','leavesdb','resources','leavesdb.db')\n",
    "\n",
    "local_db = leavesdb.init_local_db()\n",
    "\n",
    "print(local_db)\n",
    "\n",
    "db = dataset.connect(f'sqlite:///{local_db}', row_type=stuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.** Print a summary of the database's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Database column keys': ['id', 'specie', 'genus', 'path', 'family', 'dataset'], 'distinct datasets': ['Fossil', 'Leaves', 'PNAS', 'plant_village'], 'Number of distinct families': [('Fossil', 27), ('Leaves', 376), ('PNAS', 19), ('plant_village', 3)], 'Number of rows in db': 119084}\n"
     ]
    }
   ],
   "source": [
    "db_summary = leavesdb.summarize_db(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.** Select a subset of datasets\n",
    "##### Here we select the Fossil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = leavesdb.db_query.load_Fossil_data(db)\n",
    "\n",
    "# data = leavesdb.db_query.load_data(db, dataset=DATASET)\n",
    "\n",
    "# data = leavesdb.db_query.load_Leaves_data(db)\n",
    "\n",
    "\n",
    "data = leavesdb.db_query.load_all_data(db)\n",
    "\n",
    "data_by_dataset = data.groupby(by='dataset')\n",
    "data_by_dataset_dict = {k:v for k,v in data_by_dataset}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                path   family        dataset\n",
      "0  /media/data_cifs/jacob/data/plantvillage/Apple...  Rosacea  plant_village\n",
      "1  /media/data_cifs/jacob/data/plantvillage/Apple...  Rosacea  plant_village\n",
      "2  /media/data_cifs/jacob/data/plantvillage/Apple...  Rosacea  plant_village\n",
      "3  /media/data_cifs/jacob/data/plantvillage/Apple...  Rosacea  plant_village\n",
      "4  /media/data_cifs/jacob/data/plantvillage/Apple...  Rosacea  plant_village\n"
     ]
    }
   ],
   "source": [
    "print(data.head(5))\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fossil 6122 6122\n",
      "Converted image 4 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0335 Sambucus newtoni.png\n",
      "True\n",
      "Converted image 0 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0141 Sambucus newtoni.png\n",
      "True\n",
      "Converted image 8 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0338cu Sambucus newtoni.png\n",
      "True\n",
      "Converted image 6 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0338 Sambucus newtoni.png\n",
      "True\n",
      "Converted image 7 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0338cpt Sambucus newtoni.png\n",
      "True\n",
      "Converted image 3 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0224 Sambucus newtoni.png\n",
      "Converted image 1 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0141cpt Sambucus newtoni.png\n",
      "Converted image 5 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0335cu Sambucus newtoni.png\n",
      "True\n",
      "TrueTrue\n",
      "\n",
      "Converted image 2 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0141cptcu Sambucus newtoni.png\n",
      "True\n",
      "Converted image 9 and saved at /media/data/jacob/Fossil_Project/Fossil/Adoxaceae/CU_0338cu1 Sambucus newtoni.png\n",
      "True\n",
      "Finished copying 10 from Fossil in 13.467 at a rate of 0.743\n",
      "Leaves 26982 26982\n",
      "Converted image 3 and saved at /media/data/jacob/Fossil_Project/Leaves/Acanthaceae/Ax990.png\n",
      "True\n",
      "Converted image 2 and saved at /media/data/jacob/Fossil_Project/Leaves/Acanthaceae/Ax989.png\n",
      "True\n",
      "Converted image 4 and saved at /media/data/jacob/Fossil_Project/Leaves/Acanthaceae/Ax991.png\n",
      "Converted image 1 and saved at /media/data/jacob/Fossil_Project/Leaves/Acanthaceae/Ax988.png\n",
      "TrueTrue\n",
      "\n",
      "Converted image 0 and saved at /media/data/jacob/Fossil_Project/Leaves/Acanthaceae/Ax917.png\n",
      "True\n",
      "Converted image 9 and saved at /media/data/jacob/Fossil_Project/Leaves/Aceraceae/ax142.png\n",
      "True\n",
      "Converted image 5 and saved at /media/data/jacob/Fossil_Project/Leaves/Aceraceae/ax110.png\n",
      "True\n",
      "Converted image 8 and saved at /media/data/jacob/Fossil_Project/Leaves/Aceraceae/ax134.png\n",
      "True\n",
      "Converted image 7 and saved at /media/data/jacob/Fossil_Project/Leaves/Aceraceae/ax133.png\n",
      "True\n",
      "Converted image 6 and saved at /media/data/jacob/Fossil_Project/Leaves/Aceraceae/ax113.png\n",
      "True\n",
      "Finished copying 10 from Leaves in 5.138 at a rate of 1.946\n",
      "PNAS 5314 5314\n",
      "Converted image 0 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Actinocheita_filicina_4199 {WolfeUSGS} [0.60x].png\n",
      "True\n",
      "Converted image 1 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Anacardium_microsepalum_4202 {WolfeUSGS} [0.37x].png\n",
      "True\n",
      "Converted image 2 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Anacardium_pumilum_4201 {WolfeUSGS} [0.32x].png\n",
      "True\n",
      "Converted image 6 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Bouea_oppositifolia_1755 {WolfeUSGS} [0.38x].png\n",
      "True\n",
      "Converted image 3 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Androtium_astylum_8175 {WolfeUSGS} [0.50x].png\n",
      "True\n",
      "Converted image 4 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Astronium_graveolens_835 {WolfeUSGS} [1.96x].png\n",
      "True\n",
      "Converted image 7 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Campnosperma_brevipetiolata_4304 {WolfeUSGS} [1.96x].png\n",
      "True\n",
      "Converted image 5 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Blepharocarya_involucrigera_4798 {WolfeUSGS} [1.96x].png\n",
      "True\n",
      "Converted image 8 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Campnosperma_minus_8193 {WolfeUSGS} [1.96x].png\n",
      "True\n",
      "Converted image 9 and saved at /media/data/jacob/Fossil_Project/PNAS/Anacardiaceae/Anacardiaceae_Comocladia_glabra_07-080-06 {Klucking} [0.85x].png\n",
      "True\n",
      "Finished copying 10 from PNAS in 0.306 at a rate of 32.632\n",
      "plant_village 6129 6129\n",
      "Converted image 0 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/0055dd26-23a7-4415-ac61-e0b44ebfaf80___RS_HL 5672_final_masked.png\n",
      "True\n",
      "Converted image 9 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/0181db4e-718e-41bc-b6ed-92863471fa85___RS_HL 5941_final_masked.png\n",
      "True\n",
      "Converted image 7 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/013b7c70-5e3b-42b7-86af-167815a5b04f___RS_HL 7480_final_masked.pngConverted image 6 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/011d02f3-5c3c-4484-a384-b1a0a0dbdec1___RS_HL 7544_final_masked.png\n",
      "Converted image 1 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/00907d8b-6ae6-4306-bfd7-d54471981a86___RS_HL 5709_final_masked.png\n",
      "TrueConverted image 5 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/010125c0-e6f2-4f3d-b469-dd98ffecbb01___RS_HL 7948_final_masked.png\n",
      "True\n",
      "\n",
      "True\n",
      "Converted image 4 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/00fca0da-2db3-481b-b98a-9b67bb7b105c___RS_HL 7708_final_masked.png\n",
      "Converted image 2 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/0098dbd9-286a-4d6a-bf4b-5459d66f88c0___RS_HL 5776_final_masked.png\n",
      "True\n",
      "True\n",
      "\n",
      "True\n",
      "Converted image 8 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/017fd21b-142f-4988-849a-5fd316ea9a1b___RS_HL 6279_final_masked.png\n",
      "True\n",
      "Converted image 3 and saved at /media/data/jacob/Fossil_Project/plant_village/Rosacea/00a6039c-e425-4f7d-81b1-d6b0e668517e___RS_HL 7669_final_masked.png\n",
      "True\n",
      "Finished copying 10 from plant_village in 0.310 at a rate of 32.232\n"
     ]
    }
   ],
   "source": [
    "new_data_locations = {}\n",
    "for dataset_name, rows in data_by_dataset_dict.items():\n",
    "    filepaths = list(rows['path'].values)\n",
    "    labels = list(rows['family'].values)\n",
    "    print(dataset_name, len(filepaths), len(labels))\n",
    "    \n",
    "    \n",
    "    filepaths = filepaths[:10]\n",
    "    labels = labels[:10]\n",
    "    \n",
    "    num_files = len(filepaths)\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    new_dataset_paths = convert_to_png(filepaths, labels, dataset_name = dataset_name)\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time-start_time\n",
    "    print(f'Finished copying {num_files} from {dataset_name} in {total_time:.3f} at a rate of {num_files/total_time:.3f} images/sec')\n",
    "    new_dataset_paths = list(new_dataset_paths)\n",
    "    new_data_locations.update({dataset_name:new_dataset_paths})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.** Encode labels as integers for feeding into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyleaves.data_pipeline.preprocessing import encode_labels\n",
    "\n",
    "data_df = encode_labels(data)\n",
    "\n",
    "data_df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyleaves.data_pipeline.preprocessing import filter_low_count_labels, one_hot_encode_labels, one_hot_decode_labels\n",
    "\n",
    "test_size = 0.25\n",
    "val_size = 0.25\n",
    "\n",
    "data_df = filter_low_count_labels(data_df, threshold=3, verbose = True)\n",
    "\n",
    "data_df = encode_labels(data_df) #Re-encode numeric labels after removing sub-threshold classes so that max(labels) == len(labels)\n",
    "\n",
    "image_paths = data_df['path'].values.reshape((-1,1))\n",
    "one_hot_labels = one_hot_encode_labels(data_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data = train_test_split(data_df, test_size=test_size, random_state=random_seed, shuffle=True, stratify=data_df['label'])\n",
    "train_paths, test_paths, train_labels, test_labels  = train_test_split(image_paths, one_hot_labels, test_size=test_size, random_state=random_seed, shuffle=True, stratify=data_df['label'])\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=val_size, random_state=random_seed, shuffle=True, stratify=train_labels)\n",
    "\n",
    "\n",
    "train_data = {'path': train_paths, 'label': train_labels}\n",
    "val_data = {'path': val_paths, 'label': val_labels}\n",
    "test_data = {'path': test_paths, 'label': test_labels}\n",
    "\n",
    "data_splits = {'train': train_data,\n",
    "              'val': val_data,\n",
    "              'test': test_data}\n",
    "\n",
    "# train_gen = get_multiprocess_dataflow(train_data['path'], train_data['label'], size=(299,299), batch_size=32, num_prefetch=25, num_proc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's set up our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_frequencies = leavesdb.utils.plot_class_frequencies\n",
    "    \n",
    "plot_class_frequencies(labels=one_hot_decode_labels(train_data['label']).ravel().tolist());\n",
    "plot_class_frequencies(labels=one_hot_decode_labels(val_data['label']).ravel().tolist());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(data_df['label']))\n",
    "img_size = [299,299]\n",
    "channels = 3\n",
    "batch_size = 32\n",
    "learning_rate=0.01\n",
    "num_epochs = 1\n",
    "\n",
    "def parse_function(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.io.decode_jpeg(img, channels=channels)#, dtype=tf.float32)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img, label #{'image':img, 'label':label}\n",
    "\n",
    "# def train_preprocess(img, label):\n",
    "#     img = tf.image.resize(img, img_size)\n",
    "#     return {'image':img, 'label':label}\n",
    "    \n",
    "\n",
    "def get_tf_dataset(filenames, labels):\n",
    "    data = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    data = data.shuffle(len(filenames))\n",
    "#     data = data.interleave((lambda x, y: tf.data.Dataset(x,y).map(parse_function, num_parallel_calls=1)), cycle_length=4, block_length=16)\n",
    "    data = data.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.map(train_preprocess, num_parallel_calls=4)\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0'))\n",
    "    return data\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "def debug_parse_function(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.io.decode_jpeg(img, channels=channels)#, dtype=tf.float32)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img, label, filename #{'image':img, 'label':label}\n",
    "\n",
    "\n",
    "\n",
    "def debug_get_tf_dataset(filenames, labels):\n",
    "    data = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    data = data.shuffle(len(filenames))\n",
    "#     data = data.interleave((lambda x, y: tf.data.Dataset(x,y).map(parse_function, num_parallel_calls=1)), cycle_length=4, block_length=16)\n",
    "    data = data.map(debug_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.map(train_preprocess, num_parallel_calls=4)\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    data = data.cache()\n",
    "#     data = data.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0'))\n",
    "    return data\n",
    "\n",
    "##############################\n",
    "\n",
    "debug = False#True\n",
    "\n",
    "if debug == True:\n",
    "    get_tf_dataset = debug_get_tf_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_labels(data_df):\n",
    "    data_df=data_df.groupby('label', group_keys=False).apply(lambda df: df.sample(1).loc[:,['label','family']])\n",
    "    data_df.sort_values(by='label', inplace=True)\n",
    "    data_df.set_index(keys='label',drop=True, inplace=True)\n",
    "    data_df = data_df.to_dict()\n",
    "    \n",
    "    return data_df['family']\n",
    "\n",
    "\n",
    "# train_dataset = get_tf_dataset(filenames = train_data['path'].values, labels = train_data['label'].values)\n",
    "# val_dataset = get_tf_dataset(filenames = val_data['path'].values, labels = val_data['label'].values)\n",
    "\n",
    "train_dataset = get_tf_dataset(filenames = train_data['path'].ravel(), labels = train_data['label'])\n",
    "val_dataset = get_tf_dataset(filenames = val_data['path'].ravel(), labels = val_data['label'])\n",
    "\n",
    "label_map = decode_labels(data_df=data_df)\n",
    "\n",
    "num_samples_train = len(train_data['path'])\n",
    "num_samples_val = len(val_data['path'])\n",
    "num_samples_test = len(test_data['path'])\n",
    "print(num_samples_train)\n",
    "print(num_samples_val)\n",
    "print(num_samples_test)\n",
    "\n",
    "label_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for features in  train_dataset.take(1):\n",
    "#     image_batch = features[0].numpy().astype(np.int)\n",
    "#     label_batch = features[1].numpy().astype(np.int)\n",
    "\n",
    "# plot_image_grid = pyleaves.analysis.img_utils.plot_image_grid\n",
    "    \n",
    "    \n",
    "# plot_image_grid(image_batch, label_batch, x_plots = 4, y_plots = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "\n",
    "\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# train_data['label'] = train_data['label'].astype(str)\n",
    "\n",
    "# datagen_flow = datagen.flow_from_dataframe(train_data.iloc[:100,:], x_col='path', y_col='label', class_mode='sparse', batch_size=batch_size)\n",
    "\n",
    "# a=next(datagen_flow)\n",
    "\n",
    "# print(a[0].shape, a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "# n=100\n",
    "# total_time = 0\n",
    "# try:\n",
    "#     for i, features in enumerate(train_dataset.take(n)):\n",
    "# #         print(i, features[0].shape, features[1].shape)\n",
    "#         run_time = time.time()-start_time\n",
    "#         total_time += run_time\n",
    "#         print(f'Took {run_time:.2f} seconds')\n",
    "#         start_time = time.time()\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print(f'finished {i} iterations')\n",
    "\n",
    "# avg_time = total_time / i+1\n",
    "\n",
    "# rate = (i+1)*batch_size/total_time\n",
    "\n",
    "# print(f'Avg time = {avg_time:.2f} | Ran {i+1} iterations using batch size = {batch_size} & {batch_size*n} samples')\n",
    "# print(f'rate = {rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyleaves.models.inception_v3.build_model(num_classes, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = pyleaves.models.inception_v3.train_model(model,\n",
    "# \t\t\t\ttrain_dataset,\n",
    "# \t\t\t\tvalidation_data=val_dataset, \n",
    "# \t\t\t\tsteps_per_epoch=int(num_samples_train//batch_size),\n",
    "# \t\t\t\tvalidation_steps=int(num_samples_val//batch_size),\n",
    "# \t\t\t\tmax_epochs=num_epochs,\n",
    "# # \t\t\t\tcallbacks=None,\n",
    "# \t\t\t\tworkers=5,\n",
    "# \t\t\t\tinitial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.make_initializable_iterator().get_next()\n",
    "# val_dataset = val_dataset.make_initializable_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_epoch = 0\n",
    "# while current_epoch < 20:\n",
    "#     try:\n",
    "#         history = model.fit(\n",
    "#                         train_dataset,\n",
    "#                         validation_data=val_dataset, \n",
    "#         #                 steps_per_epoch=int(num_samples_train//batch_size),\n",
    "#         #                 validation_steps=int(num_samples_val//batch_size),\n",
    "#                         epochs=num_epochs,\n",
    "#                         # \t\t\t\tcallbacks=None,\n",
    "#                         workers=10,\n",
    "#                         initial_epoch=current_epoch,\n",
    "#                         verbose=1)\n",
    "#     except KeyboardInterrupt:\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(f'current epoch = {current_epoch}, error: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data = tf.get_default_session().run(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.python.framework.errors_impl import InvalidArgumentError\n",
    "\n",
    "\n",
    "filename_ids = []\n",
    "batch_log = []\n",
    "invalid_filenames = []\n",
    "\n",
    "start_time = time.time()\n",
    "time_log = []\n",
    "steps_per_epoch = num_samples_train//batch_size\n",
    "\n",
    "valid_filenames = []\n",
    "\n",
    "reset_iter = True\n",
    "current_epoch = 0\n",
    "while current_epoch < 20:\n",
    "    if reset_iter == True:\n",
    "        epoch_dataset = train_dataset.take(steps_per_epoch)\n",
    "        reset_iter = False\n",
    "    try:\n",
    "        for i, (imgs, labels, filenames) in enumerate(epoch_dataset):        \n",
    "            run_time = time.time()-start_time\n",
    "            time_log.append(run_time)\n",
    "            print(f'Took {run_time:.2f} seconds')\n",
    "            \n",
    "            valid_filenames.append([fname.numpy().decode('utf-8') for fname in filenames])\n",
    "            \n",
    "            start_time = time.time()\n",
    "    except InvalidArgumentError as e:\n",
    "        invalid_flag = 0\n",
    "        for j, fname in enumerate(filenames):\n",
    "            fname = fname.numpy().decode('utf-8')\n",
    "            if os.path.isfile(fname):\n",
    "                filename_ids.append(i*batch_size+j)\n",
    "                valid_filenames.append(fname)\n",
    "                continue\n",
    "            else:\n",
    "                filename_ids.append(i*batch_size+j)\n",
    "                invalid_filenames.append(fname)    \n",
    "                print(f'invalid filename = {fname}')\n",
    "                invalid_flag = 1\n",
    "        print(f'current epoch = {current_epoch}, error: {e}', type(e))\n",
    "        continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        reset_iter = True\n",
    "        print(f'current epoch = {current_epoch}, error: {e}', type(e))\n",
    "            \n",
    "print(f'finished {i*batch_size} samples over {i} iterations in {np.sum(time_log):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invalid_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "plot_image_grid = pyleaves.analysis.img_utils.plot_image_grid\n",
    "\n",
    "invalid_filenames = np.concatenate([fname.numpy().tolist() for fname in invalid_filenames]).tolist()\n",
    "\n",
    "for i, fname in enumerate(invalid_filenames):\n",
    "    if type(fname)==bytes:\n",
    "        invalid_filenames[i] = fname.decode('utf-8')    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_imgs = []\n",
    "for i, fname in enumerate(invalid_filenames):\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.resize(img, tuple(img_size))\n",
    "    invalid_imgs.append(img)\n",
    "\n",
    "    \n",
    "invalid_images = np.stack(invalid_imgs)\n",
    "\n",
    "plot_image_grid(invalid_images, labels = np.ones(len(invalid_imgs)), x_plots = 4, y_plots = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

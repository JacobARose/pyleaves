{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, navigate to your chosen working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/..\n",
    "%pwd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 34\n",
    "import random\n",
    "random.seed(random_seed)\n",
    "import dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stuf import stuf\n",
    "\n",
    "import os\n",
    "gpu=5\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6\"#str(gpu)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf;\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf_config=tf.ConfigProto(log_device_placement=True)\n",
    "tf_config.gpu_options.allocator_type = 'BFC'\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.allow_soft_placement = True\n",
    "sess = tf.Session(config=tf_config)\n",
    "K.set_session(sess)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "print(get_available_gpus())\n",
    "\n",
    "import pyleaves\n",
    "\n",
    "from pyleaves import leavesdb\n",
    "from pyleaves.data_pipeline.tensorpack_loaders import get_multiprocess_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tf.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.** Initialize and connect to database in local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_db = leavesdb.init_local_db()\n",
    "\n",
    "db = dataset.connect(f'sqlite:///{local_db}', row_type=stuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.** Print a summary of the database's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leavesdb.summarize_db(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.** Select a subset of datasets\n",
    "##### Here we select the Fossil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = leavesdb.db_query.load_Fossil_data(db)\n",
    "\n",
    "data = leavesdb.db_query.load_Leaves_data(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.** Encode labels as integers for feeding into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyleaves.data_pipeline.preprocessing import encode_labels\n",
    "\n",
    "data_df = encode_labels(data)\n",
    "\n",
    "data_df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyleaves.data_pipeline.preprocessing import filter_low_count_labels, one_hot_encode_labels, one_hot_decode_labels\n",
    "\n",
    "test_size = 0.25\n",
    "val_size = 0.25\n",
    "\n",
    "data_df = filter_low_count_labels(data_df, threshold=3, verbose = True)\n",
    "\n",
    "data_df = encode_labels(data_df) #Re-encode numeric labels after removing sub-threshold classes so that max(labels) == len(labels)\n",
    "\n",
    "image_paths = data_df['path'].values.reshape((-1,1))\n",
    "one_hot_labels = one_hot_encode_labels(data_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data = train_test_split(data_df, test_size=test_size, random_state=random_seed, shuffle=True, stratify=data_df['label'])\n",
    "train_paths, test_paths, train_labels, test_labels  = train_test_split(image_paths, one_hot_labels, test_size=test_size, random_state=random_seed, shuffle=True, stratify=data_df['label'])\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=val_size, random_state=random_seed, shuffle=True, stratify=train_labels)\n",
    "\n",
    "\n",
    "train_data = {'path': train_paths, 'label': train_labels}\n",
    "val_data = {'path': val_paths, 'label': val_labels}\n",
    "test_data = {'path': test_paths, 'label': test_labels}\n",
    "\n",
    "data_splits = {'train': train_data,\n",
    "              'val': val_data,\n",
    "              'test': test_data}\n",
    "\n",
    "# train_gen = get_multiprocess_dataflow(train_data['path'], train_data['label'], size=(299,299), batch_size=32, num_prefetch=25, num_proc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's set up our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_frequencies = leavesdb.utils.plot_class_frequencies\n",
    "    \n",
    "plot_class_frequencies(labels=one_hot_decode_labels(train_data['label']).ravel().tolist());\n",
    "plot_class_frequencies(labels=one_hot_decode_labels(val_data['label']).ravel().tolist());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(data_df['label']))\n",
    "img_size = [299,299]\n",
    "channels = 3\n",
    "batch_size = 32\n",
    "learning_rate=0.01\n",
    "num_epochs = 1\n",
    "\n",
    "def parse_function(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.io.decode_jpeg(img, channels=channels)#, dtype=tf.float32)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img, label #{'image':img, 'label':label}\n",
    "\n",
    "# def train_preprocess(img, label):\n",
    "#     img = tf.image.resize(img, img_size)\n",
    "#     return {'image':img, 'label':label}\n",
    "    \n",
    "\n",
    "def get_tf_dataset(filenames, labels):\n",
    "    data = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    data = data.shuffle(len(filenames))\n",
    "#     data = data.interleave((lambda x, y: tf.data.Dataset(x,y).map(parse_function, num_parallel_calls=1)), cycle_length=4, block_length=16)\n",
    "    data = data.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.map(train_preprocess, num_parallel_calls=4)\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0'))\n",
    "    return data\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "def debug_parse_function(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.io.decode_jpeg(img, channels=channels)#, dtype=tf.float32)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img, label, filename #{'image':img, 'label':label}\n",
    "\n",
    "\n",
    "\n",
    "def debug_get_tf_dataset(filenames, labels):\n",
    "    data = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    data = data.shuffle(len(filenames))\n",
    "#     data = data.interleave((lambda x, y: tf.data.Dataset(x,y).map(parse_function, num_parallel_calls=1)), cycle_length=4, block_length=16)\n",
    "    data = data.map(debug_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.map(train_preprocess, num_parallel_calls=4)\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    data = data.cache()\n",
    "#     data = data.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0'))\n",
    "    return data\n",
    "\n",
    "##############################\n",
    "\n",
    "debug = False#True\n",
    "\n",
    "if debug == True:\n",
    "    get_tf_dataset = debug_get_tf_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_labels(data_df):\n",
    "    data_df=data_df.groupby('label', group_keys=False).apply(lambda df: df.sample(1).loc[:,['label','family']])\n",
    "    data_df.sort_values(by='label', inplace=True)\n",
    "    data_df.set_index(keys='label',drop=True, inplace=True)\n",
    "    data_df = data_df.to_dict()\n",
    "    \n",
    "    return data_df['family']\n",
    "\n",
    "\n",
    "# train_dataset = get_tf_dataset(filenames = train_data['path'].values, labels = train_data['label'].values)\n",
    "# val_dataset = get_tf_dataset(filenames = val_data['path'].values, labels = val_data['label'].values)\n",
    "\n",
    "train_dataset = get_tf_dataset(filenames = train_data['path'].ravel(), labels = train_data['label'])\n",
    "val_dataset = get_tf_dataset(filenames = val_data['path'].ravel(), labels = val_data['label'])\n",
    "\n",
    "label_map = decode_labels(data_df=data_df)\n",
    "\n",
    "num_samples_train = len(train_data['path'])\n",
    "num_samples_val = len(val_data['path'])\n",
    "num_samples_test = len(test_data['path'])\n",
    "print(num_samples_train)\n",
    "print(num_samples_val)\n",
    "print(num_samples_test)\n",
    "\n",
    "label_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for features in  train_dataset.take(1):\n",
    "#     image_batch = features[0].numpy().astype(np.int)\n",
    "#     label_batch = features[1].numpy().astype(np.int)\n",
    "\n",
    "# plot_image_grid = pyleaves.analysis.img_utils.plot_image_grid\n",
    "    \n",
    "    \n",
    "# plot_image_grid(image_batch, label_batch, x_plots = 4, y_plots = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "\n",
    "\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# train_data['label'] = train_data['label'].astype(str)\n",
    "\n",
    "# datagen_flow = datagen.flow_from_dataframe(train_data.iloc[:100,:], x_col='path', y_col='label', class_mode='sparse', batch_size=batch_size)\n",
    "\n",
    "# a=next(datagen_flow)\n",
    "\n",
    "# print(a[0].shape, a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "# n=100\n",
    "# total_time = 0\n",
    "# try:\n",
    "#     for i, features in enumerate(train_dataset.take(n)):\n",
    "# #         print(i, features[0].shape, features[1].shape)\n",
    "#         run_time = time.time()-start_time\n",
    "#         total_time += run_time\n",
    "#         print(f'Took {run_time:.2f} seconds')\n",
    "#         start_time = time.time()\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print(f'finished {i} iterations')\n",
    "\n",
    "# avg_time = total_time / i+1\n",
    "\n",
    "# rate = (i+1)*batch_size/total_time\n",
    "\n",
    "# print(f'Avg time = {avg_time:.2f} | Ran {i+1} iterations using batch size = {batch_size} & {batch_size*n} samples')\n",
    "# print(f'rate = {rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyleaves.models.inception_v3.build_model(num_classes, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = pyleaves.models.inception_v3.train_model(model,\n",
    "# \t\t\t\ttrain_dataset,\n",
    "# \t\t\t\tvalidation_data=val_dataset, \n",
    "# \t\t\t\tsteps_per_epoch=int(num_samples_train//batch_size),\n",
    "# \t\t\t\tvalidation_steps=int(num_samples_val//batch_size),\n",
    "# \t\t\t\tmax_epochs=num_epochs,\n",
    "# # \t\t\t\tcallbacks=None,\n",
    "# \t\t\t\tworkers=5,\n",
    "# \t\t\t\tinitial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.make_initializable_iterator().get_next()\n",
    "# val_dataset = val_dataset.make_initializable_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_epoch = 0\n",
    "# while current_epoch < 20:\n",
    "#     try:\n",
    "#         history = model.fit(\n",
    "#                         train_dataset,\n",
    "#                         validation_data=val_dataset, \n",
    "#         #                 steps_per_epoch=int(num_samples_train//batch_size),\n",
    "#         #                 validation_steps=int(num_samples_val//batch_size),\n",
    "#                         epochs=num_epochs,\n",
    "#                         # \t\t\t\tcallbacks=None,\n",
    "#                         workers=10,\n",
    "#                         initial_epoch=current_epoch,\n",
    "#                         verbose=1)\n",
    "#     except KeyboardInterrupt:\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(f'current epoch = {current_epoch}, error: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data = tf.get_default_session().run(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.python.framework.errors_impl import InvalidArgumentError\n",
    "\n",
    "\n",
    "filename_ids = []\n",
    "batch_log = []\n",
    "invalid_filenames = []\n",
    "\n",
    "start_time = time.time()\n",
    "time_log = []\n",
    "steps_per_epoch = num_samples_train//batch_size\n",
    "\n",
    "valid_filenames = []\n",
    "\n",
    "reset_iter = True\n",
    "current_epoch = 0\n",
    "while current_epoch < 20:\n",
    "    if reset_iter == True:\n",
    "        epoch_dataset = train_dataset.take(steps_per_epoch)\n",
    "        reset_iter = False\n",
    "    try:\n",
    "        for i, (imgs, labels, filenames) in enumerate(epoch_dataset):        \n",
    "            run_time = time.time()-start_time\n",
    "            time_log.append(run_time)\n",
    "            print(f'Took {run_time:.2f} seconds')\n",
    "            \n",
    "            valid_filenames.append([fname.numpy().decode('utf-8') for fname in filenames])\n",
    "            \n",
    "            start_time = time.time()\n",
    "    except InvalidArgumentError as e:\n",
    "        invalid_flag = 0\n",
    "        for j, fname in enumerate(filenames):\n",
    "            fname = fname.numpy().decode('utf-8')\n",
    "            if os.path.isfile(fname):\n",
    "                filename_ids.append(i*batch_size+j)\n",
    "                valid_filenames.append(fname)\n",
    "                continue\n",
    "            else:\n",
    "                filename_ids.append(i*batch_size+j)\n",
    "                invalid_filenames.append(fname)    \n",
    "                print(f'invalid filename = {fname}')\n",
    "                invalid_flag = 1\n",
    "        print(f'current epoch = {current_epoch}, error: {e}', type(e))\n",
    "        continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        reset_iter = True\n",
    "        print(f'current epoch = {current_epoch}, error: {e}', type(e))\n",
    "            \n",
    "print(f'finished {i*batch_size} samples over {i} iterations in {np.sum(time_log):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invalid_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "plot_image_grid = pyleaves.analysis.img_utils.plot_image_grid\n",
    "\n",
    "invalid_filenames = np.concatenate([fname.numpy().tolist() for fname in invalid_filenames]).tolist()\n",
    "\n",
    "for i, fname in enumerate(invalid_filenames):\n",
    "    if type(fname)==bytes:\n",
    "        invalid_filenames[i] = fname.decode('utf-8')    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_imgs = []\n",
    "for i, fname in enumerate(invalid_filenames):\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.resize(img, tuple(img_size))\n",
    "    invalid_imgs.append(img)\n",
    "\n",
    "    \n",
    "invalid_images = np.stack(invalid_imgs)\n",
    "\n",
    "plot_image_grid(invalid_images, labels = np.ones(len(invalid_imgs)), x_plots = 4, y_plots = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, navigate to your chosen working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/..\n",
    "%pwd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:88:00.0, compute capability: 6.1\n",
      "\n",
      "['/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "random_seed = 34\n",
    "import random\n",
    "random.seed(random_seed)\n",
    "import dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from stuf import stuf\n",
    "\n",
    "import os\n",
    "gpu=5\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"#str(gpu)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf;\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "tf_config=tf.ConfigProto(log_device_placement=True)\n",
    "tf_config.gpu_options.allocator_type = 'BFC'\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "tf_config.allow_soft_placement = True\n",
    "sess = tf.Session(config=tf_config)\n",
    "K.set_session(sess)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "\n",
    "print(get_available_gpus())\n",
    "\n",
    "import pyleaves\n",
    "\n",
    "from pyleaves import leavesdb\n",
    "from pyleaves.data_pipeline.tensorpack_loaders import get_multiprocess_dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = \"Fossil\"\n",
    "# tf.test.gpu_device_name()\n",
    "# dir(tf.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.** Initialize and connect to database in local filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with sql db at location /home/jacob/scripts/leavesdb.db\n",
      "/home/jacob/scripts/leavesdb.db\n"
     ]
    }
   ],
   "source": [
    "local_db = leavesdb.init_local_db()\n",
    "\n",
    "print(local_db)\n",
    "\n",
    "db = dataset.connect(f'sqlite:///{local_db}', row_type=stuf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.** Print a summary of the database's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database column keys:\n",
      " ['id', 'specie', 'genus', 'path', 'family', 'dataset']\n",
      "Number of distinct families:\n",
      " [('Fossil', 27), ('Leaves', 376), ('PNAS', 19), ('plant_village', 3)]\n",
      "Number of rows in db:\n",
      " 119084\n"
     ]
    }
   ],
   "source": [
    "leavesdb.summarize_db(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.** Select a subset of datasets\n",
    "##### Here we select the Fossil dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = leavesdb.db_query.load_Fossil_data(db)\n",
    "\n",
    "data = leavesdb.db_query.load_data(db, dataset=DATASET)\n",
    "\n",
    "# data = leavesdb.db_query.load_Leaves_data(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dataset.util.ResultIter'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.** Encode labels as integers for feeding into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>family</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3802</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>II. IDs, families uncertain</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Sapindaceae</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Ulmaceae</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Berberidaceae</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1145</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Fagaceae</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5075</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Unidentified</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Unidentified</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2721</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Ulmaceae</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Rosaceae</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5527</th>\n",
       "      <td>/media/data_cifs/sven2/leaves/sorted/Fossils_D...</td>\n",
       "      <td>Unidentified</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   path  \\\n",
       "3802  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "2400  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "3592  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "280   /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "1145  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "5075  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "4971  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "2721  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "2170  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "5527  /media/data_cifs/sven2/leaves/sorted/Fossils_D...   \n",
       "\n",
       "                           family  label  \n",
       "3802  II. IDs, families uncertain     12  \n",
       "2400                  Sapindaceae     22  \n",
       "3592                     Ulmaceae     24  \n",
       "280                 Berberidaceae      4  \n",
       "1145                     Fagaceae      9  \n",
       "5075                 Unidentified     25  \n",
       "4971                 Unidentified     25  \n",
       "2721                     Ulmaceae     24  \n",
       "2170                     Rosaceae     20  \n",
       "5527                 Unidentified     25  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyleaves.data_pipeline.preprocessing import encode_labels\n",
    "\n",
    "data_df = encode_labels(data)\n",
    "\n",
    "data_df.sample(frac=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6122\n"
     ]
    }
   ],
   "source": [
    "print(len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting only samples that belong to a class with population >= 3 samples\n",
      "Previous num_classes = 27, new num_classes = 24\n",
      "Previous data_df.shape = (6122, 3), new data_df.shape = (6118, 3)\n"
     ]
    }
   ],
   "source": [
    "from pyleaves.data_pipeline.preprocessing import filter_low_count_labels, one_hot_encode_labels, one_hot_decode_labels\n",
    "\n",
    "test_size = 0.25\n",
    "val_size = 0.25\n",
    "\n",
    "data_df = filter_low_count_labels(data_df, threshold=3, verbose = True)\n",
    "\n",
    "data_df = encode_labels(data_df) #Re-encode numeric labels after removing sub-threshold classes so that max(labels) == len(labels)\n",
    "\n",
    "image_paths = data_df['path'].values.reshape((-1,1))\n",
    "one_hot_labels = one_hot_encode_labels(data_df['label'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data = train_test_split(data_df, test_size=test_size, random_state=random_seed, shuffle=True, stratify=data_df['label'])\n",
    "train_paths, test_paths, train_labels, test_labels  = train_test_split(image_paths, one_hot_labels, test_size=test_size, random_state=random_seed, shuffle=True, stratify=data_df['label'])\n",
    "\n",
    "train_paths, val_paths, train_labels, val_labels = train_test_split(train_paths, train_labels, test_size=val_size, random_state=random_seed, shuffle=True, stratify=train_labels)\n",
    "\n",
    "\n",
    "train_data = {'path': train_paths, 'label': train_labels}\n",
    "val_data = {'path': val_paths, 'label': val_labels}\n",
    "test_data = {'path': test_paths, 'label': test_labels}\n",
    "\n",
    "data_splits = {'train': train_data,\n",
    "              'val': val_data,\n",
    "              'test': test_data}\n",
    "\n",
    "# train_gen = get_multiprocess_dataflow(train_data['path'], train_data['label'], size=(299,299), batch_size=32, num_prefetch=25, num_proc=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Let's set up our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_frequencies = leavesdb.utils.plot_class_frequencies\n",
    "    \n",
    "plot_class_frequencies(labels=one_hot_decode_labels(train_data['label']).ravel().tolist());\n",
    "plot_class_frequencies(labels=one_hot_decode_labels(val_data['label']).ravel().tolist());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(data_df['label']))\n",
    "img_size = [299,299]\n",
    "channels = 3\n",
    "batch_size = 32\n",
    "learning_rate=0.01\n",
    "num_epochs = 1\n",
    "\n",
    "def parse_function(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.io.decode_jpeg(img, channels=channels)#, dtype=tf.float32)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img, label #{'image':img, 'label':label}\n",
    "\n",
    "# def train_preprocess(img, label):\n",
    "#     img = tf.image.resize(img, img_size)\n",
    "#     return {'image':img, 'label':label}\n",
    "    \n",
    "\n",
    "def get_tf_dataset(filenames, labels):\n",
    "    data = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    data = data.shuffle(len(filenames))\n",
    "#     data = data.interleave((lambda x, y: tf.data.Dataset(x,y).map(parse_function, num_parallel_calls=1)), cycle_length=4, block_length=16)\n",
    "    data = data.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.map(train_preprocess, num_parallel_calls=4)\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0'))\n",
    "    return data\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "def debug_parse_function(filename, label):\n",
    "    img = tf.io.read_file(filename)\n",
    "    img = tf.io.decode_jpeg(img, channels=channels)#, dtype=tf.float32)\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    return img, label, filename #{'image':img, 'label':label}\n",
    "\n",
    "\n",
    "\n",
    "def debug_get_tf_dataset(filenames, labels):\n",
    "    data = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    data = data.shuffle(len(filenames))\n",
    "#     data = data.interleave((lambda x, y: tf.data.Dataset(x,y).map(parse_function, num_parallel_calls=1)), cycle_length=4, block_length=16)\n",
    "    data = data.map(debug_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "#     data = data.map(train_preprocess, num_parallel_calls=4)\n",
    "    data = data.batch(batch_size)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    data = data.cache()\n",
    "#     data = data.apply(tf.data.experimental.prefetch_to_device('/device:GPU:0'))\n",
    "    return data\n",
    "\n",
    "##############################\n",
    "\n",
    "debug = False#True\n",
    "\n",
    "if debug == True:\n",
    "    get_tf_dataset = debug_get_tf_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_labels(data_df):\n",
    "    data_df=data_df.groupby('label', group_keys=False).apply(lambda df: df.sample(1).loc[:,['label','family']])\n",
    "    data_df.sort_values(by='label', inplace=True)\n",
    "    data_df.set_index(keys='label',drop=True, inplace=True)\n",
    "    data_df = data_df.to_dict()\n",
    "    \n",
    "    return data_df['family']\n",
    "\n",
    "\n",
    "# train_dataset = get_tf_dataset(filenames = train_data['path'].values, labels = train_data['label'].values)\n",
    "# val_dataset = get_tf_dataset(filenames = val_data['path'].values, labels = val_data['label'].values)\n",
    "\n",
    "train_dataset = get_tf_dataset(filenames = train_data['path'].ravel(), labels = train_data['label'])\n",
    "val_dataset = get_tf_dataset(filenames = val_data['path'].ravel(), labels = val_data['label'])\n",
    "\n",
    "label_map = decode_labels(data_df=data_df)\n",
    "\n",
    "num_samples_train = len(train_data['path'])\n",
    "num_samples_val = len(val_data['path'])\n",
    "num_samples_test = len(test_data['path'])\n",
    "print(num_samples_train)\n",
    "print(num_samples_val)\n",
    "print(num_samples_test)\n",
    "\n",
    "label_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for features in  train_dataset.take(1):\n",
    "#     image_batch = features[0].numpy().astype(np.int)\n",
    "#     label_batch = features[1].numpy().astype(np.int)\n",
    "\n",
    "# plot_image_grid = pyleaves.analysis.img_utils.plot_image_grid\n",
    "    \n",
    "    \n",
    "# plot_image_grid(image_batch, label_batch, x_plots = 4, y_plots = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "\n",
    "\n",
    "# datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# train_data['label'] = train_data['label'].astype(str)\n",
    "\n",
    "# datagen_flow = datagen.flow_from_dataframe(train_data.iloc[:100,:], x_col='path', y_col='label', class_mode='sparse', batch_size=batch_size)\n",
    "\n",
    "# a=next(datagen_flow)\n",
    "\n",
    "# print(a[0].shape, a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "\n",
    "# start_time = time.time()\n",
    "# n=100\n",
    "# total_time = 0\n",
    "# try:\n",
    "#     for i, features in enumerate(train_dataset.take(n)):\n",
    "# #         print(i, features[0].shape, features[1].shape)\n",
    "#         run_time = time.time()-start_time\n",
    "#         total_time += run_time\n",
    "#         print(f'Took {run_time:.2f} seconds')\n",
    "#         start_time = time.time()\n",
    "# except Exception as e:\n",
    "#     print(e)\n",
    "#     print(f'finished {i} iterations')\n",
    "\n",
    "# avg_time = total_time / i+1\n",
    "\n",
    "# rate = (i+1)*batch_size/total_time\n",
    "\n",
    "# print(f'Avg time = {avg_time:.2f} | Ran {i+1} iterations using batch size = {batch_size} & {batch_size*n} samples')\n",
    "# print(f'rate = {rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyleaves.models.inception_v3.build_model(num_classes, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = pyleaves.models.inception_v3.train_model(model,\n",
    "# \t\t\t\ttrain_dataset,\n",
    "# \t\t\t\tvalidation_data=val_dataset, \n",
    "# \t\t\t\tsteps_per_epoch=int(num_samples_train//batch_size),\n",
    "# \t\t\t\tvalidation_steps=int(num_samples_val//batch_size),\n",
    "# \t\t\t\tmax_epochs=num_epochs,\n",
    "# # \t\t\t\tcallbacks=None,\n",
    "# \t\t\t\tworkers=5,\n",
    "# \t\t\t\tinitial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = train_dataset.make_initializable_iterator().get_next()\n",
    "# val_dataset = val_dataset.make_initializable_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_epoch = 0\n",
    "# while current_epoch < 20:\n",
    "#     try:\n",
    "#         history = model.fit(\n",
    "#                         train_dataset,\n",
    "#                         validation_data=val_dataset, \n",
    "#         #                 steps_per_epoch=int(num_samples_train//batch_size),\n",
    "#         #                 validation_steps=int(num_samples_val//batch_size),\n",
    "#                         epochs=num_epochs,\n",
    "#                         # \t\t\t\tcallbacks=None,\n",
    "#                         workers=10,\n",
    "#                         initial_epoch=current_epoch,\n",
    "#                         verbose=1)\n",
    "#     except KeyboardInterrupt:\n",
    "#         break\n",
    "#     except Exception as e:\n",
    "#         print(f'current epoch = {current_epoch}, error: {e}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read_data = tf.get_default_session().run(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.python.framework.errors_impl import InvalidArgumentError\n",
    "\n",
    "\n",
    "filename_ids = []\n",
    "batch_log = []\n",
    "invalid_filenames = []\n",
    "\n",
    "start_time = time.time()\n",
    "time_log = []\n",
    "steps_per_epoch = num_samples_train//batch_size\n",
    "\n",
    "valid_filenames = []\n",
    "\n",
    "reset_iter = True\n",
    "current_epoch = 0\n",
    "while current_epoch < 20:\n",
    "    if reset_iter == True:\n",
    "        epoch_dataset = train_dataset.take(steps_per_epoch)\n",
    "        reset_iter = False\n",
    "    try:\n",
    "        for i, (imgs, labels, filenames) in enumerate(epoch_dataset):        \n",
    "            run_time = time.time()-start_time\n",
    "            time_log.append(run_time)\n",
    "            print(f'Took {run_time:.2f} seconds')\n",
    "            \n",
    "            valid_filenames.append([fname.numpy().decode('utf-8') for fname in filenames])\n",
    "            \n",
    "            start_time = time.time()\n",
    "    except InvalidArgumentError as e:\n",
    "        invalid_flag = 0\n",
    "        for j, fname in enumerate(filenames):\n",
    "            fname = fname.numpy().decode('utf-8')\n",
    "            if os.path.isfile(fname):\n",
    "                filename_ids.append(i*batch_size+j)\n",
    "                valid_filenames.append(fname)\n",
    "                continue\n",
    "            else:\n",
    "                filename_ids.append(i*batch_size+j)\n",
    "                invalid_filenames.append(fname)    \n",
    "                print(f'invalid filename = {fname}')\n",
    "                invalid_flag = 1\n",
    "        print(f'current epoch = {current_epoch}, error: {e}', type(e))\n",
    "        continue\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "    except Exception as e:\n",
    "        reset_iter = True\n",
    "        print(f'current epoch = {current_epoch}, error: {e}', type(e))\n",
    "            \n",
    "print(f'finished {i*batch_size} samples over {i} iterations in {np.sum(time_log):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invalid_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "plot_image_grid = pyleaves.analysis.img_utils.plot_image_grid\n",
    "\n",
    "invalid_filenames = np.concatenate([fname.numpy().tolist() for fname in invalid_filenames]).tolist()\n",
    "\n",
    "for i, fname in enumerate(invalid_filenames):\n",
    "    if type(fname)==bytes:\n",
    "        invalid_filenames[i] = fname.decode('utf-8')    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_imgs = []\n",
    "for i, fname in enumerate(invalid_filenames):\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.resize(img, tuple(img_size))\n",
    "    invalid_imgs.append(img)\n",
    "\n",
    "    \n",
    "invalid_images = np.stack(invalid_imgs)\n",
    "\n",
    "plot_image_grid(invalid_images, labels = np.ones(len(invalid_imgs)), x_plots = 4, y_plots = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

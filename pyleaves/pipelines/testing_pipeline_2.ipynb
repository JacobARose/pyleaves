{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from hydra.experimental import compose, initialize\n",
    "import hydra\n",
    "from copy import copy\n",
    "from omegaconf import OmegaConf\n",
    "from box import Box, BoxList\n",
    "import os\n",
    "from datetime import datetime\n",
    "from itertools import product\n",
    "from pprint import pprint\n",
    "\n",
    "logs = BoxList([])\n",
    "seed = 78\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "from pyleaves.utils import ensure_dir_exists\n",
    "\n",
    "# overrides = ['dataset@dataset=PNAS_family_100', 'model@model=resnet_50_v2', 'model.params.optimizer=\"SGD\"', 'model.params.weights=\"imagenet\"', 'model.params.lr=1e-5', 'model.params.regularization.l2=1e-3', 'callbacks.early_stopping.patience=3', 'tags=[\"PNAS_family_100\",\"debug\",\"env=pyleaves2.2\"]', 'dataset.params.training.target_size=[512,512]', 'dataset.params.training.batch_size=16', 'model.params.head_layers=[512,256]', 'dataset.params.training.num_epochs=30', 'orchestration.debug=True']#,  '--multirun']\n",
    "\n",
    "def main(config_name=\"simplified_config\", overrides: list=[]):\n",
    "\n",
    "    with initialize(config_path=\"configs\"):\n",
    "        config = compose(config_name=config_name, overrides=overrides)\n",
    "\n",
    "    return config\n",
    "# config = main(overrides=overrides)\n",
    "\n",
    "# config = main(overrides=['dataset@dataset=Leaves_family_100'])\n",
    "# config = main(overrides=['dataset@dataset=PNAS_family_100'])\n",
    "\n",
    "# initialize(config_path=\"configs\")\n",
    "# config_name=\"simplified_config\"\n",
    "# # config_0 = compose(config_name=config_name, overrides=overrides)\n",
    "\n",
    "# overrides=['dataset@dataset=PNAS_family_100']\n",
    "# config = compose(config_name=config_name, overrides=overrides)\n",
    "\n",
    "# test_config = main(overrides=['dataset@dataset=Fossil_family_100_test',\n",
    "#                               '~dataset.params.training.target_size',\n",
    "#                              'dataset.params.training.augmentations=[]'])\n",
    "# # merged_config = OmegaConf.merge(config, test_config)\n",
    "# # copied = OmegaConf.masked_copy(test_config, [\"dataset\"])\n",
    "# # pprint(OmegaConf.to_container(copied))\n",
    "# # pprint(OmegaConf.to_container(test_config))\n",
    "\n",
    "# def resolve_test_config(main_config, test_data_config):\n",
    "\n",
    "#     copied_test_cfg = OmegaConf.masked_copy(test_data_config, [\"dataset\"])\n",
    "#     resolved_main_cfg = OmegaConf.to_container(main_config, resolve=True)\n",
    "#     resolved_main_cfg = OmegaConf.create(resolved_main_cfg)\n",
    "#     merged_config = OmegaConf.merge(resolved_main_cfg, copied_test_cfg)\n",
    "    \n",
    "#     return merged_config\n",
    "\n",
    "# config.dataset.params.training\n",
    "\n",
    "# config.task = 0\n",
    "# config.dataset.params.training.target_size = [768,768]\n",
    "# merged_config = resolve_test_config(main_config=config, test_data_config=test_config)\n",
    "\n",
    "# # merged_config.task = 0\n",
    "# pprint(OmegaConf.to_container(merged_config, resolve=True))\n",
    "\n",
    "# type(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programmatically transferring images into class-specific hierarchical directories\n",
    "\n",
    "### Transforming a 2-level hierarchy into single level in preparation for using an ImageDataGenerator from tf.keras\n",
    "\n",
    "\n",
    "\n",
    "We have a dataset of images from 3 different collections (i.e. original publishing source) with overlapping subsets of class labels in the form of biological family (e.g. Theaceae, Rosaceae).\n",
    "\n",
    "## source structure:\n",
    "```\n",
    "./wolfe/\n",
    "        Acanthaceae/\n",
    "            image_0.jpg\n",
    "            image_1.jpg\n",
    "            ...\n",
    "        Achariaceae/\n",
    "        ...\n",
    "        Winteraceae/\n",
    "        Zygophyllaceae/\n",
    "./axelrod/\n",
    "        Acanthaceae/\n",
    "            ...\n",
    "        ...\n",
    "./klucking/\n",
    "        ...\n",
    "```\n",
    "\n",
    "\n",
    "## target structure:\n",
    "\n",
    "```\n",
    "Acanthaceae/\n",
    "    image_0.jpg\n",
    "    image_1.jpg\n",
    "    ...\n",
    "Achariaceae/\n",
    "...\n",
    "Winteraceae/\n",
    "Zygophyllaceae/\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jacob/envs/pyleaves2.2/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d54aaecf664aba965780c8f1a646d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='PNAS collections', max=3.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e614752dbac4ffdaa2060da0d838998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='collection wolfe', max=164.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f6d82cad7e04d4a84aa17e239f49846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='collection axelrod', max=52.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b2a8453cef41b29385c06f9741dfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='collection klucking', max=72.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "# PNAS_dir = '/media/data_cifs/jacob/Fossil_Project/data/scratch_data/PNAS 2016 leaves'#/PNAS_family'\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "PNAS_dir = '/media/data_cifs_lrs/projects/prj_fossils/data/processed_data/PNAS_2020-06'\n",
    "output_root_dir = '/media/data_cifs_lrs/projects/prj_fossils/data/processed_data/PNAS_2020-06/PNAS_family'\n",
    "collections = ['wolfe','axelrod','klucking']\n",
    "collection_dirs = [os.path.join(PNAS_dir, collection) for collection in collections]\n",
    "collection_dirs\n",
    "\n",
    "def get_collection_files(collection_dir, position=0, leave=False):\n",
    "    collection_files = {}\n",
    "    for family in tqdm(os.listdir(collection_dir), desc=f'collection {Path(collection_dir).name}', position=position, leave=leave):\n",
    "        family_dir = os.path.join(collection_dir, family)\n",
    "        fam_files = [os.path.join(family_dir, file) for file in os.listdir(family_dir)]\n",
    "        if family not in collection_files.keys():\n",
    "            collection_files[family] = []\n",
    "        collection_files[family].extend(fam_files)\n",
    "        \n",
    "    return collection_files\n",
    "    \n",
    "def gather_collections(collection_dirs, leave=False):\n",
    "    all_files = {}\n",
    "    for collection_dir in tqdm(collection_dirs, desc=f'PNAS collections', position=0, leave=leave):\n",
    "        collection_files = get_collection_files(collection_dir, position=1, leave=leave)\n",
    "        for family, fam_files in collection_files.items():\n",
    "            if family not in all_files.keys():\n",
    "                all_files[family] = []\n",
    "            all_files[family].extend(fam_files)\n",
    "    return all_files\n",
    "\n",
    "all_files = gather_collections(collection_dirs, leave=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a21cc73fad14083bdbd737cfd66d65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Family directories', max=182.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Trochodendraceae files', max=1.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3400b42a3db34628b6b428798fb70fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Fabaceae files', max=776.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2b9a70cb1105>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfamily_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfamily_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{family} files'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/data/conda/jacob/envs/pyleaves2.2/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jacob/envs/pyleaves2.2/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/data/conda/jacob/envs/pyleaves2.2/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "leave=True\n",
    "for family, family_files in  tqdm(all_files.items(), desc='Family directories', position=0, leave=leave):\n",
    "    family_dir = os.path.join(output_root_dir, family)\n",
    "    os.makedirs(family_dir)\n",
    "    for file in tqdm(family_files, desc = f'{family} files', position=1, leave=False):\n",
    "        shutil.copy(file, family_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################################################################################################################################################\n",
    "################################################################################################################################################\n",
    "# NEW SECTION\n",
    "################################################################################################################################################\n",
    "################################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from sklearn.metrics import classification_report\n",
    "# import os\n",
    "# import numpy as np\n",
    "# # model_path = \"/media/data/jacob/simplified-baselines/Leaves_in_PNAS_family_100_resnet_50_v2_[512, 512]/task-9_2020-09-22_23-25-32/model_dir/saved_model\"\n",
    "# # model = tf.keras.models.load_model(model_path)\n",
    "# model_config.num_classes = encoder.num_classes\n",
    "# model = build_model(model_config)\n",
    "\n",
    "# # from hydra.experimental import compose, initialize\n",
    "# from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# # with initialize(config_path=\"configs\"):\n",
    "# #     config = compose(config_name=\"config\", overrides=['dataset@dataset=PNAS','use_tfrecords=False'])\n",
    "# #     print(config.pretty())\n",
    "# import hydra\n",
    "# # from pyleaves.pipelines.pipeline_1 import *\n",
    "\n",
    "# from pyleaves.datasets import base_dataset\n",
    "\n",
    "# from pyleaves.utils.experiment_utils import resolve_config_interpolations\n",
    "# from paleoai_data.utils.kfold_cross_validation import DataFold\n",
    "# from typing import List, Union\n",
    "# import random\n",
    "# import numpy as np\n",
    "# from more_itertools import unzip\n",
    "# import shutil\n",
    "# import os\n",
    "# import neptune\n",
    "# from pathlib import Path\n",
    "# import yaml\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/data/conda/jacob/envs/pyleaves2.2/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: [2]\n",
      "Initial visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Successfully set memory_growth=True and limited GPUs visible to tensorflow.\n",
      "\n",
      "Now using GPU(s):\n",
      "['/physical_device:GPU:0']\n",
      "batch_size: 32\n",
      "color_mode: rgb\n",
      "early_stopping:\n",
      "  min_delta: 0.01\n",
      "  monitor: val_loss\n",
      "  patience: 10\n",
      "  restore_best_weights: true\n",
      "image_dir: /media/data_cifs_lrs/projects/prj_fossils/data/processed_data/PNAS_2020-06/PNAS_family\n",
      "log_dir: /media/data_cifs_lrs/projects/prj_fossils/users/jacob/tensorboard_log_dir\n",
      "num_epochs: 10\n",
      "preprocess_input: tensorflow.keras.applications.resnet_v2.preprocess_input\n",
      "rescale: null\n",
      "seed: 20\n",
      "target_size: !!python/tuple\n",
      "- 299\n",
      "- 299\n",
      "validation_split: 0.1\n",
      "\n",
      "Found 456 images belonging to 2 classes.\n",
      "Found 50 images belonging to 2 classes.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50v2 (Model)           (None, 10, 10, 2048)      23564800  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense0 (Dense)               (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 24,122,498\n",
      "Trainable params: 24,031,618\n",
      "Non-trainable params: 90,880\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-fd7aedcd9b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0mneptune_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneptune_experiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fd7aedcd9b1c>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m \u001b[0mneptune_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneptune_experiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def summarize_sample(x, y):\n",
    "    y_int=y\n",
    "    y_encoding = 'sparse int'\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y_int = np.argmax(y, axis=-1)\n",
    "        if y.ndim>=1 and y.shape[-1] > 1:\n",
    "            y_encoding = 'one hot'\n",
    "    print(f'y = {y_int} [{y_encoding} encoded]')\n",
    "    print(f'y.dtype = {y.dtype}, x.dtype = {x.dtype}\\n')\n",
    "    print(f'y.shape = {y.shape},\\ny.min() = {y.min():.3f} | y.max() = {y.max():.3f},\\ny.mean() = {y.mean():.3f} | y.std() = {y.std():.3f}\\n')\n",
    "    print(f'x.shape = {x.shape},\\nx.min() = {x.min():.3f} | x.max() = {x.max():.3f},\\nx.mean() = {x.mean():.3f} | x.std() = {x.std():.3f}')\n",
    "\n",
    "    plt.imshow(x)\n",
    "\n",
    "# from pyleaves.pipelines.pipeline_simple import *\n",
    "# from hydra.core.hydra_config import HydraConfig\n",
    "from pyleaves.utils import set_tf_config\n",
    "gpu = set_tf_config(gpu_num=None, num_gpus=1, wait=0)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "from pyleaves.utils.pipeline_utils import build_model\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
    "from pprint import pprint\n",
    "from box import Box\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import neptune\n",
    "# import neptune_tensorboard as neptune_tb\n",
    "\n",
    "from neptunecontrib.monitoring.keras import NeptuneMonitor\n",
    "\n",
    "# neptune_tb.integrate_with_tensorflow()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     featurewise_center=False, samplewise_center=False,\n",
    "#     featurewise_std_normalization=False, samplewise_std_normalization=False,\n",
    "#     zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0,\n",
    "#     height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0,\n",
    "#     channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False,\n",
    "#     vertical_flip=False, rescale=1.0/255, preprocessing_function=preprocess_input,\n",
    "#     data_format=None, validation_split=validation_split, dtype=np.uint8\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "neptune_project_name = 'jacobarose/jupyter-testing-ground'\n",
    "\n",
    "params = Box({\n",
    "              'image_dir': '/media/data_cifs_lrs/projects/prj_fossils/data/processed_data/PNAS_2020-06/PNAS_family',\n",
    "              'log_dir': '/media/data_cifs_lrs/projects/prj_fossils/users/jacob/tensorboard_log_dir',          \n",
    "              'validation_split': 0.1,\n",
    "              'target_size':(299,299),\n",
    "              'batch_size':32,\n",
    "              'num_epochs': 10,\n",
    "              'seed': 20,\n",
    "              'rescale': None, #1.0/255,\n",
    "              'preprocess_input': \"tensorflow.keras.applications.resnet_v2.preprocess_input\", #None,\n",
    "              'color_mode': 'rgb',\n",
    "              'early_stopping': {'monitor':\"val_loss\",\n",
    "                                'patience':10,\n",
    "                                'min_delta':0.01,\n",
    "                                'restore_best_weights':True}\n",
    "    \n",
    "})\n",
    "\n",
    "# from omegaconf import OmegaConf\n",
    "print(params.to_yaml())\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=params.rescale,\n",
    "                                                          preprocessing_function=params.preprocess_input,\n",
    "                                                          validation_split=params.validation_split)\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    params.image_dir, target_size=params.target_size, color_mode=params.color_mode, classes=None,\n",
    "    class_mode='categorical', batch_size=params.batch_size, shuffle=True, seed=params.seed,\n",
    "    subset='training', interpolation='nearest')\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    params.image_dir, target_size=params.target_size, color_mode=params.color_mode, classes=None,\n",
    "    class_mode='categorical', batch_size=params.batch_size, shuffle=False, seed=params.seed,\n",
    "    subset='validation', interpolation='nearest')\n",
    "\n",
    "params.num_samples_train = train_data.samples\n",
    "params.num_samples_val = val_data.samples\n",
    "params.num_classes = train_data.num_classes\n",
    "steps_per_epoch=params.num_samples_train//params.batch_size\n",
    "validation_steps=params.num_samples_val//params.batch_size\n",
    "model_config = Box({\n",
    "                    'model_name': \"resnet_50_v2\",\n",
    "                    'optimizer':\"Adam\",\n",
    "                    'num_classes':params.num_classes,\n",
    "                    'weights': \"imagenet\",\n",
    "                    'frozen_layers':'bn', #None, #(0,-4),\n",
    "                    'input_shape':(*params.target_size,3),\n",
    "                    'lr':1e-5,\n",
    "                    'lr_momentum':None,#0.9,\n",
    "                    'regularization':{},#{\"l2\": 1e-4},\n",
    "                    'loss':'categorical_crossentropy',\n",
    "                    'METRICS':['f1','accuracy'],\n",
    "                    'head_layers': [256,128]\n",
    "                    })\n",
    "\n",
    "neptune.init(project_qualified_name=neptune_project_name)\n",
    "\n",
    "\n",
    "\n",
    "model = build_model(model_config)\n",
    "model.summary()\n",
    "\n",
    "# base = model.layers[0]\n",
    "# dir([l for l in base.layers if l.name.endswith('bn')][0])\n",
    "# def freeze_batchnorm_layers(model, verbose=False):\n",
    "#     for i, layer in enumerate(model.layers):\n",
    "#         if layer.name.endswith('bn'):\n",
    "#             if verbose: print(f'layer {i}', layer.name)\n",
    "#             layer.trainable = False\n",
    "#     return model\n",
    "\n",
    "# base = freeze_batchnorm_layers(base, verbose=True)\n",
    "\n",
    "# len(base.layers)\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "callbacks = [TensorBoard(log_dir=params.log_dir, profile_batch=0),\n",
    "             NeptuneMonitor(),\n",
    "             EarlyStopping(monitor=params.early_stopping.monitor,\n",
    "                           patience=params.early_stopping.patience,\n",
    "                           min_delta=params.early_stopping.min_delta, \n",
    "                           verbose=1, \n",
    "                           restore_best_weights=params.early_stopping.restore_best_weights)]\n",
    "\n",
    "\n",
    "neptune_params = {k: str(v) for k,v in params.items() if type(v)==dict}\n",
    "\n",
    "with neptune.create_experiment(name=neptune_experiment_name, params=params):\n",
    "    model.summary(print_fn=lambda x: neptune.log_text('model_summary', x))\n",
    "\n",
    "    print('[BEGINNING TRAINING]')\n",
    "    try:\n",
    "        history = model.fit(train_data,\n",
    "                            epochs=num_epochs,\n",
    "                            callbacks=callbacks,\n",
    "                            validation_data=val_data,\n",
    "                            validation_freq=1,\n",
    "                            shuffle=True,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_steps=validation_steps,\n",
    "                            verbose=1)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Box: {'image_dir': '/media/data_cifs_lrs/projects/prj_fossils/data/processed_data/PNAS_2020-06/PNAS_family', 'log_dir': '/media/data_cifs_lrs/projects/prj_fossils/users/jacob/tensorboard_log_dir', 'validation_split': 0.1, 'target_size': (299, 299), 'batch_size': 32, 'num_epochs': 10, 'seed': 20, 'rescale': None, 'preprocess_input': 'tensorflow.keras.applications.resnet_v2.preprocess_input', 'color_mode': 'rgb', 'early_stopping': {'monitor': 'val_loss', 'patience': 10, 'min_delta': 0.01, 'restore_best_weights': True}, 'num_samples_train': 456, 'num_samples_val': 50, 'num_classes': 2}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if os.path.exists(csv_path):\n",
    "        neptune.log_artifact(csv_path)\n",
    "\n",
    "    model.save(config.run_dirs.saved_model_path)\n",
    "    print('[STAGE COMPLETED]')\n",
    "    print(f'Saved trained model to {config.run_dirs.saved_model_path}')\n",
    "\n",
    "    print('history.history.keys() =',history.history.keys())\n",
    "\n",
    "    steps = split_datasets['test'].num_samples//data_config.training.batch_size\n",
    "\n",
    "    test_results = evaluate(model, encoder, model_config, data_config, test_data=test_data, steps=steps, num_classes=encoder.num_classes, confusion_matrix=True)\n",
    "\n",
    "    print('TEST RESULTS:')\n",
    "    pprint(test_results)\n",
    "\n",
    "    # for k,v in test_results.items():\n",
    "    #     neptune.log_metric(k, v)\n",
    "    # predictions = model.predict(test_data, steps=split_datasets['test'].num_samples)\n",
    "\n",
    "print(['[FINISHED TRAINING AND TESTING]'])\n",
    "\n",
    "# return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.task = 0\n",
    "# data_config = config.dataset.params\n",
    "# extract_config = config.dataset.params.extract\n",
    "# training_config = config.dataset.params.training\n",
    "# model_config = config.model.params\n",
    "# preprocess_config = config.model.params.preprocess_input\n",
    "# model_config.input_shape = (*training_config.target_size, extract_config.num_channels)\n",
    "# config.model.params = model_config\n",
    "\n",
    "##############################################\n",
    "# test_stage_config = init_Fossil_family_100_test_config(main_config=config)\n",
    "# test_data_config = test_stage_config.dataset.params\n",
    "# test_fold_dir = test_data_config.extract.fold_dir\n",
    "# test_fold_id = test_data_config.extract.fold_id\n",
    "# test_fold_path = DataFold.query_fold_dir(test_fold_dir, test_fold_id)\n",
    "# fossil_test_fold = DataFold.from_artifact_path(test_fold_path)\n",
    "##############################################\n",
    "\n",
    "# fold_path = DataFold.query_fold_dir(extract_config.fold_dir, extract_config.fold_id)\n",
    "# fold = DataFold.from_artifact_path(fold_path)\n",
    "# ##############################################\n",
    "\n",
    "# encoder = init_pipeline_encoder_scheme(fold, test_fold=fossil_test_fold, scheme = config.pipeline.encoding_scheme, threshold=100)\n",
    "\n",
    "# data, extracted_data, split_datasets, encoder = create_dataset(data_fold=fold,\n",
    "#                                                                data_config=data_config,\n",
    "#                                                                preprocess_config=preprocess_config,\n",
    "#                                                                encoder=encoder,\n",
    "#                                                                cache=True,\n",
    "#                                                                cache_image_dir=config.run_dirs.cache_dir,\n",
    "#                                                                seed=config.misc.seed)\n",
    "\n",
    "# report = classification_report(y, y_hat, target_names=text_labels, output_dict=True)\n",
    "\n",
    "# split_datasets['train'].class_distribution\n",
    "\n",
    "# from typing import Dict\n",
    "\n",
    "# def calc_class_weights(class_distribution: Dict[str,int], num_samples: int):\n",
    "#     num_classes = len(class_distribution)\n",
    "#     weights = (1/num_classes)*(num_samples/np.array(list(class_distribution.values())))\n",
    "    \n",
    "#     return weights\n",
    "\n",
    "\n",
    "# train_dataset = split_datasets['train']\n",
    "\n",
    "# w = calc_class_weights(class_distribution=train_dataset.class_distribution, num_samples=train_dataset.num_samples)\n",
    "\n",
    "# w\n",
    "\n",
    "# train_dataset = split_datasets['train']\n",
    "# class_distribution = train_dataset.class_distribution\n",
    "# # class_distribution.pop('Fabaceae')\n",
    "\n",
    "# def calc_class_weights(class_distribution: Dict[str,int], num_samples: int, encoder=None, use_int_keys=True):\n",
    "\n",
    "#     mandatory = encoder.classes\n",
    "#     class_names = list(class_distribution.keys())[:16]\n",
    "#     absent = set(mandatory).union(set(class_names)) - set(class_names).intersection(mandatory)\n",
    "\n",
    "#     full_distribution = {**class_distribution, **{class_i:0 for class_i in absent}}\n",
    "#     full_distribution = {k:full_distribution[k] for k in sorted(full_distribution)}\n",
    "#     if use_int_keys:\n",
    "#         full_distribution = {encoder.encode([l])[0]:count for l, count in full_distribution.items()}\n",
    "\n",
    "    \n",
    "    \n",
    "#     num_classes = len(full_distribution)\n",
    "#     weights = (1/num_classes)*(num_samples/np.array(list(full_distribution.values())))\n",
    "#     return weights\n",
    "\n",
    "# use_int_keys=True\n",
    "# mandatory = encoder.classes\n",
    "# class_names = list(class_distribution.keys())[:16]\n",
    "# absent = set(mandatory).union(set(class_names)) - set(class_names).intersection(mandatory)\n",
    "\n",
    "# full_distribution = {**class_distribution, **{class_i:0 for class_i in absent}}\n",
    "# full_distribution = {k:full_distribution[k] for k in sorted(full_distribution)}\n",
    "# if use_int_keys:\n",
    "#     full_distribution = {encoder.encode([l])[0]:count for l, count in full_distribution.items()}\n",
    "\n",
    "\n",
    "# pprint([(k,full_distribution[k]) for k in sorted(full_distribution)])\n",
    "\n",
    "# {k:full_distribution[k] for k in sorted(full_distribution)}\n",
    "\n",
    "# help(sorted)\n",
    "\n",
    "# for k, v in full_distribution.items():\n",
    "#     print(k, v)\n",
    "\n",
    "# mandatory\n",
    "# absent\n",
    "\n",
    "# calc_class_weights(class_distribution=train_dataset.class_distribution, num_samples=train_dataset.num_samples, include_classes)\n",
    "\n",
    "# class_distribution\n",
    "\n",
    "# split_datasets['train'].class_distribution\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# report_pd = pd.DataFrame(report)\n",
    "\n",
    "# report_pd\n",
    "\n",
    "\n",
    "# num_test_samples = split_datasets['test'].num_samples\n",
    "# steps = num_test_samples//test_data_config.training.batch_size\n",
    "# x = data['test']\n",
    "# y = x.map(lambda x,y: y).take(steps).unbatch().batch(num_test_samples)\n",
    "\n",
    "\n",
    "\n",
    "# y_collect = []\n",
    "\n",
    "# for i, y_i in enumerate(y):\n",
    "#     print(i)\n",
    "#     y_collect.append(y_i)\n",
    "    \n",
    "#     if i>4000:\n",
    "#         print('ending early')\n",
    "#         break\n",
    "    \n",
    "# print(len(y_collect), y_collect[0])\n",
    "\n",
    "# import numpy as np\n",
    "# y_np = np.vstack(y_collect[0])\n",
    "# # y_collect[:5]\n",
    "\n",
    "# y_np.sum(axis=0)\n",
    "\n",
    "# import dataclass\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class DataExtractConfig:\n",
    "\n",
    "#     fold_id: null\n",
    "#     fold_dir: null\n",
    "#     dataset_name: null\n",
    "#     num_classes: null\n",
    "#     threshold: 0\n",
    "#     color_mode: 'grayscale'\n",
    "#     num_channels: 3\n",
    "#     val_split: 0.1\n",
    "#     include_classes: []\n",
    "#     exclude_classes: ['notcataloged','notcatalogued', 'II. IDs, families uncertain', 'Unidentified']\n",
    "\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class DatasetConfig:\n",
    "    \n",
    "# #This should be added in the defaults of the global config\n",
    "# params:\n",
    "\n",
    "\n",
    "#   training:\n",
    "#     augmentations:\n",
    "#       - flip: 1.0\n",
    "#     target_size: [512,512]\n",
    "#     batch_size: 16\n",
    "#     buffer_size: 200\n",
    "#     num_epochs: 150\n",
    "#     steps_per_epoch: null\n",
    "#     validation_steps: null\n",
    "#     use_tfrecords: false\n",
    "#     samples_per_shard: 300\n",
    "\n",
    "# # pprint(json.dumps(OmegaConf.to_container(config)))\n",
    "\n",
    "# # config = main()#config_name=\"2stage_simplified_config\")\n",
    "\n",
    "# config = main(config_name=\"demo_simplified\")\n",
    "\n",
    "# pprint(OmegaConf.to_container(config))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

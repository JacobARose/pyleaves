defaults:

    - dataset/base
    - model/base
    - dataset@dataset: Leaves-PNAS
    - model@model: resnet_50_v2
    - db@db : sqlite

    - hydra/launcher: joblib
    
    
# #hydra/launcher.n_jobs: 2

# dataset.params.extract.fold_dir:
# #     "/media/data/jacob/Fossil_Project/data/csv_data/paleoai_data_disk_cache_dir/v0_2/data/staged_data/${dataset.params.dataset_name}_family_100/ksplit_2"
#         "/media/data/jacob/Fossil_Project/data/csv_data/paleoai_data_disk_cache_dir/staged_data/${dataset.dataset_name}/ksplit_2/"
task: ${hydra:job.num}

pipeline:
    encoding_scheme: "{train}"
    # Specify which classes will be used to construct the label encoder
    # Possible schemes:
    #     1. "{train}"
    #     2. "{train}U{test}"
    #     3. "{train}n{test}"
    #     4. "{test}"


    stage_1:
        task: "fit+validate"
        subsets: ["train","val"]
        params:
            fit_class_weights: false #Include class_weights in args input to model.fit()
    stage_2: 
        task: "evaluate"
        subsets: ["test"]
    stage_3: 
        task: "evaluate"
        subsets: ["test"]
        dataset_name: "Fossil_family_100"
        tfrecord_dir: "/media/data/jacob/experimental_data/tfrecords/${pipeline.stage_3.dataset_name}"


misc:
    experiment_start_time: null
    # neptune_project_name: "jacobarose/simplified-baselines"
    # neptune_experiment_dir: "/media/data/jacob/simplified-baselines"
    neptune_project_name: "jacobarose/refined-baselines"
    neptune_experiment_dir: "/media/data/jacob/refined-baselines"
    run_description: ""
    experiment_name: ${dataset.params.extract.dataset_name}_${model.params.model_name}_${dataset.params.training.target_size}
    experiment_dir: ${misc.neptune_experiment_dir}/${misc.experiment_name}/${misc.experiment_start_time}
    restore_last: True
    seed: 45

run_dirs:
    log_dir: '${misc.experiment_dir}/log_dir'
    model_dir: '${misc.experiment_dir}/model_dir'
    results_dir: '${misc.experiment_dir}/results_dir'
    saved_model_path: '${run_dirs.model_dir}/saved_model'
    checkpoints_path: '${run_dirs.model_dir}/checkpoints'
    tfrecord_dir: '/media/data/jacob/experimental_data/tfrecords/${dataset.params.extract.dataset_name}'
    cache_dir: null #${misc.experiment_dir}/cache

orchestration:
    n_jobs: 1
    num_gpus: 1
    gpu_num: ${task}
    debug: False
    wait: 5

callbacks:
    confusion_matrix: 
        log_train: True
        log_val: True
        num_batches: "all"
    # reduce_lr_on_plateau:
    #     patience: 10
    #     monitor: "val_weighted_f1"        
    early_stopping:
        patience: 15
        monitor: "val_loss"
        min_delta: 0.1
        restore_best_weights: True
    log_images: False
    log_epochs: [5,10]

debugging:
    overfit_one_batch: False # set this to True to limit the training pipeline to just repeat a single batch
    log_model_input_stats: False # Set this to True to log statistical means of the input images from the first batch for each subset

tags:
    - ${pipeline.encoding_scheme}

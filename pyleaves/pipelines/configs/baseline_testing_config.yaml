defaults:
    - hydra/launcher: joblib
    

dataset_name: "PNAS_family"
saved_model_path: "/media/data_cifs_lrs/projects/prj_fossils/users/jacob/models/${dataset_name}"
image_dir: "/media/data_cifs_lrs/projects/prj_fossils/data/processed_data/PNAS_2020-06/${dataset_name}"
log_dir: "/media/data_cifs_lrs/projects/prj_fossils/users/jacob/tensorboard_log_dir"
validation_split: 1.0e-02
target_size: [299,299]
batch_size: 32
num_epochs: 30
seed: 20
rescale: null #1.0/255,
preprocess_input: "tensorflow.keras.applications.resnet_v2.preprocess_input"
color_mode: 'rgb'
early_stopping: 
    monitor: val_loss
    patience: 10
    min_delta: 0.01
    restore_best_weights: True





model_name: "resnet_50_v2"
optimizer: "Adam"
num_classes: null
weights: "imagenet"
frozen_layers: null #(0,-4),
input_shape': null
lr: 1e-5
lr_momentum: null
regularization: null
loss: "categorical_crossentropy"
METRICS: ["f1","accuracy"]
head_layers: [256,128]
task: ${hydra:job.num}



data_augs:
    featurewise_center: false
    samplewise_center: false
    featurewise_std_normalization: false
    samplewise_std_normalization: false
    zca_whitening: false
    zca_epsilon: 1e-06
    rotation_range: 0
    width_shift_range: 0.0
    height_shift_range: 0.0
    brightness_range: null
    shear_range: 0.0
    zoom_range: 0.0
    channel_shift_range: 0.0
    fill_mode: "nearest"
    cval: 0.0
    horizontal_flip: false
    vertical_flip: false
    rescale: 1.0/255
    preprocessing_function: ${preprocess_input}
    validation_split: ${validation_split}

tags:
    - ${pipeline.encoding_scheme}




# pipeline:
#     encoding_scheme: "{train}"
    # Specify which classes will be used to construct the label encoder
    # Possible schemes:
    #     1. "{train}"
    #     2. "{train}U{test}"
    #     3. "{train}n{test}"
    #     4. "{test}"


#     stage_1:
#         task: "fit+validate"
#         subsets: ["train","val"]
#         params:
#             fit_class_weights: false #Include class_weights in args input to model.fit()
#     stage_2: 
#         task: "evaluate"
#         subsets: ["test"]
#     stage_3: 
#         task: "evaluate"
#         subsets: ["test"]
#         dataset_name: "Fossil_family_100"
#         tfrecord_dir: "/media/data/jacob/experimental_data/tfrecords/${pipeline.stage_3.dataset_name}"


# misc:
#     experiment_start_time: null
#     # neptune_project_name: "jacobarose/simplified-baselines"
#     # neptune_experiment_dir: "/media/data/jacob/simplified-baselines"
#     neptune_project_name: "jacobarose/refined-baselines"
#     neptune_experiment_dir: "/media/data/jacob/refined-baselines"
#     run_description: ""
#     experiment_name: ${dataset.params.extract.dataset_name}_${model.params.model_name}_${dataset.params.training.target_size}
#     experiment_dir: ${misc.neptune_experiment_dir}/${misc.experiment_name}/${misc.experiment_start_time}
#     restore_last: True
#     seed: 45

# run_dirs:
#     log_dir: '${misc.experiment_dir}/log_dir'
#     model_dir: '${misc.experiment_dir}/model_dir'
#     results_dir: '${misc.experiment_dir}/results_dir'
#     saved_model_path: '${run_dirs.model_dir}/saved_model'
#     checkpoints_path: '${run_dirs.model_dir}/checkpoints'
#     tfrecord_dir: '/media/data/jacob/experimental_data/tfrecords/${dataset.params.extract.dataset_name}'
#     cache_dir: null #${misc.experiment_dir}/cache

# orchestration:
#     n_jobs: 1
#     num_gpus: 1
#     gpu_num: ${task}
#     debug: False
#     wait: 5

# callbacks:
#     confusion_matrix: 
#         log_train: True
#         log_val: True
#         num_batches: "all"
#     # reduce_lr_on_plateau:
#     #     patience: 10
#     #     monitor: "val_weighted_f1"        
#     early_stopping:
#         patience: 15
#         monitor: "val_loss"
#         min_delta: 0.1
#         restore_best_weights: True
#     log_images: False
#     log_epochs: [5,10]

# debugging:
#     overfit_one_batch: False # set this to True to limit the training pipeline to just repeat a single batch
#     log_model_input_stats: False # Set this to True to log statistical means of the input images from the first batch for each subset

# tags:
#     - ${pipeline.encoding_scheme}

#This config is meant to specify parameters for training on a dataset, then fine-tuning on another dataset.

defaults:

    - dataset_0/base
    - dataset_0@dataset_0: Leaves_family_4
    - dataset_1/base
    - dataset_1@dataset_1: Fossil_family_4
    - hydra/launcher: joblib
    
task: ${hydra:job.num}

dataset:
    "0": ${dataset_0}
    "1": ${dataset_1}

dataset_name:
    "0": "${dataset.0.dataset_name}"
    "1": "${dataset.1.dataset_name}"

experiment_name: "${dataset_name.0}-${dataset_name.1}_${pretrain.target_size}-${pretrain.model_name}"

pretrain:
    stage: ${pipeline.stage_0}
    dataset: ${dataset.0}
    saved_model_path: "/media/data_cifs_lrs/projects/prj_fossils/users/jacob/models/${experiment_name}/saved_model"
    test_size: '${dataset.0.test_size}'
    image_subdir: 'test_size-${pretrain.test_size}'
    train_image_dir: "${dataset.0.image_dir}/${pretrain.image_subdir}/train"
    test_image_dir: "${dataset.0.image_dir}/${pretrain.image_subdir}/test"
    validation_split: '${dataset.0.validation_split}'
    batch_size: 32
    num_epochs: 30
    num_parallel_calls: -1
    
    model_name: "resnet_50_v2"
    optimizer: "Adam"
    num_classes: null
    weights: "imagenet"
    frozen_layers: null #(0,-4),
    input_shape: null
    lr: 3.0e-5
    lr_momentum: null
    regularization:
        l2: null
        l1: null
    loss: "categorical_crossentropy"
    METRICS: ["f1","accuracy", "balanced_accuracy"]
    head_layers: [256,128]
    target_size: [299,299]

    augmentations: #null
        flip: 0.0
        rotate: 0.0
        sbc: 0.0 #saturation, brightness, contrast
        
    preprocess_input: "tensorflow.keras.applications.resnet_v2.preprocess_input"
    color_mode: 'rgb'
    early_stopping: 
        monitor: val_loss
        patience: 10
        min_delta: 0.01
        restore_best_weights: True
        
        
        
finetune:
    stage: ${pipeline.stage_2}
    dataset: ${dataset.1}
    saved_model_path: "${pretrain.saved_model_path}"    #"/media/data_cifs_lrs/projects/prj_fossils/users/jacob/models/${dataset_name.0}-${target_size}"
    test_size: '${dataset.1.test_size}'
    image_subdir: 'test_size-${finetune.test_size}'
    train_image_dir: "${dataset.1.image_dir}/${finetune.image_subdir}/train"
    test_image_dir: "${dataset.1.image_dir}/${finetune.image_subdir}/test"
    validation_split: "${dataset.1.validation_split}"
    batch_size: ${pretrain.batch_size}
    num_epochs: 80
    num_parallel_calls: -1
    
    model_name: "resnet_50_v2"
    optimizer: "Adam"
    num_classes: ${pretrain.num_classes}
    weights: "imagenet"
    frozen_layers: [0,-4]
    input_shape: null
    lr: 1e-4
    lr_momentum: null
    regularization:
        l2: null
        l1: null
    loss: "categorical_crossentropy"
    METRICS: ["f1","accuracy", "balanced_accuracy"]
    head_layers: [256,128]
    target_size: ${pretrain.target_size}

    augmentations: #null
        flip: 0.0
        rotate: 0.0
        sbc: 0.0 #saturation, brightness, contrast
        
    preprocess_input: "tensorflow.keras.applications.resnet_v2.preprocess_input"
    color_mode: 'rgb'
    early_stopping:
        monitor: val_loss
        patience: 10
        min_delta: 0.01
        restore_best_weights: True
  
    
seed: 20
log_dir: "/media/data_cifs_lrs/projects/prj_fossils/users/jacob/tensorboard_log_dir"

pipeline:
    stage_0:
        task: "pretrain+validate"
        subsets: ["train","val"]
        dataset_name: "${dataset_name.0}"
        test_size: ${dataset.0.test_size}
        params:
            fit_class_weights: false #Include class_weights in args input to model.fit()
        class_encodings: null
    stage_1: 
        task: "evaluate"
        subsets: ["test"]
        test_size: ${dataset.0.test_size}
        class_encodings: ${pipeline.stage_0.class_encodings}
    stage_2: 
        task: "finetune"
        subsets: ["train","val"]
        dataset_name: "${dataset_name.1}"
        test_size: ${dataset.1.test_size}
        params:
            fit_class_weights: false #Include class_weights in args input to model.fit()
        class_encodings: ${pipeline.stage_0.class_encodings}
    #     dataset_name: "Fossil_family_100"
        tfrecord_dir: "/media/data/jacob/experimental_data/tfrecords/${pipeline.stage_2.dataset_name}/${pipeline.stage_2.dataset_name}"

    stage_3: 
        task: "evaluate"
        subsets: ["test"]
        dataset_name: "${dataset_name.1}"
        test_size: ${dataset.1.test_size}
        class_encodings: ${pipeline.stage_0.class_encodings}
    #     dataset_name: "Fossil_family_100"
        tfrecord_dir: "/media/data/jacob/experimental_data/tfrecords/${pipeline.stage_3.dataset_name}"

















# target_size: [299,299]
# batch_size: 32
# num_epochs: 30
# num_parallel_calls: -1

# rescale: null #1.0/255,
# preprocess_input: "tensorflow.keras.applications.resnet_v2.preprocess_input"
# color_mode: 'rgb'
# early_stopping: 
#     monitor: val_loss
#     patience: 10
#     min_delta: 0.01
#     restore_best_weights: True





# model_name: "resnet_50_v2"
# optimizer: "Adam"
# num_classes: null
# weights: "imagenet"
# frozen_layers: null #(0,-4),
# input_shape: null
# lr: 1.0e-5
# lr_momentum: null
# regularization: null
# loss: "categorical_crossentropy"
# METRICS: ["f1","accuracy"]
# head_layers: [256,128]

# augmentations: #null
#     flip: 0.0



# data_augs:
#     featurewise_center: false
#     samplewise_center: false
#     featurewise_std_normalization: false
#     samplewise_std_normalization: false
#     zca_whitening: false
#     zca_epsilon: 1e-06
#     rotation_range: 0
#     width_shift_range: 0.0
#     height_shift_range: 0.0
#     brightness_range: null
#     shear_range: 0.0
#     zoom_range: 0.0
#     channel_shift_range: 0.0
#     fill_mode: "nearest"
#     cval: 0.0
#     horizontal_flip: false
#     vertical_flip: false
#     rescale: 1.0/255
#     preprocessing_function: ${preprocess_input}
#     validation_split: ${validation_split}

tags: []
fine_tune_stage:
    perform_task: false

zero_shot_test: "Fossil_family_4"


# pipeline:
#     encoding_scheme: "{train}"
    # Specify which classes will be used to construct the label encoder
    # Possible schemes:
    #     1. "{train}"
    #     2. "{train}U{test}"
    #     3. "{train}n{test}"
    #     4. "{test}"


#     stage_1:
#         task: "fit+validate"
#         subsets: ["train","val"]
#         params:
#             fit_class_weights: false #Include class_weights in args input to model.fit()
#     stage_2: 
#         task: "evaluate"
#         subsets: ["test"]
#     stage_3: 
#         task: "evaluate"
#         subsets: ["test"]
#         dataset_name: "Fossil_family_100"
#         tfrecord_dir: "/media/data/jacob/experimental_data/tfrecords/${pipeline.stage_3.dataset_name}"


# misc:
#     experiment_start_time: null
#     # neptune_project_name: "jacobarose/simplified-baselines"
#     # neptune_experiment_dir: "/media/data/jacob/simplified-baselines"
#     neptune_project_name: "jacobarose/refined-baselines"
#     neptune_experiment_dir: "/media/data/jacob/refined-baselines"
#     run_description: ""
#     experiment_name: ${dataset.params.extract.dataset_name}_${model.params.model_name}_${dataset.params.training.target_size}
#     experiment_dir: ${misc.neptune_experiment_dir}/${misc.experiment_name}/${misc.experiment_start_time}
#     restore_last: True
#     seed: 45

# run_dirs:
#     log_dir: '${misc.experiment_dir}/log_dir'
#     model_dir: '${misc.experiment_dir}/model_dir'
#     results_dir: '${misc.experiment_dir}/results_dir'
#     saved_model_path: '${run_dirs.model_dir}/saved_model'
#     checkpoints_path: '${run_dirs.model_dir}/checkpoints'
#     tfrecord_dir: '/media/data/jacob/experimental_data/tfrecords/${dataset.params.extract.dataset_name}'
#     cache_dir: null #${misc.experiment_dir}/cache

# orchestration:
#     n_jobs: 1
#     num_gpus: 1
#     gpu_num: ${task}
#     debug: False
#     wait: 5

# callbacks:
#     confusion_matrix: 
#         log_train: True
#         log_val: True
#         num_batches: "all"
#     # reduce_lr_on_plateau:
#     #     patience: 10
#     #     monitor: "val_weighted_f1"        
#     early_stopping:
#         patience: 15
#         monitor: "val_loss"
#         min_delta: 0.1
#         restore_best_weights: True
#     log_images: False
#     log_epochs: [5,10]

# debugging:
#     overfit_one_batch: False # set this to True to limit the training pipeline to just repeat a single batch
#     log_model_input_stats: False # Set this to True to log statistical means of the input images from the first batch for each subset

# tags:
#     - ${pipeline.encoding_scheme}
